{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6dda38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c6d2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Preprocess Data\n",
    "def load_tiny_shakespeare():\n",
    "    \"\"\"\n",
    "    Simulate loading data from tiny_shakespeare.py\n",
    "    Normally you would import the module, but we'll simulate the dataset structure\n",
    "    \"\"\"\n",
    "    # This would normally be imported from tiny_shakespeare.py\n",
    "    try:\n",
    "        with open('shakespeare_clean.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        # Sample text for demonstration\n",
    "        text = \"\"\"\n",
    "        To be, or not to be, that is the question:\n",
    "        Whether 'tis nobler in the mind to suffer\n",
    "        The slings and arrows of outrageous fortune,\n",
    "        Or to take arms against a sea of troubles\n",
    "        And by opposing end them.\n",
    "        \"\"\"\n",
    "        print(\"Using sample text since tiny_shakespeare.txt was not found\")\n",
    "    \n",
    "    # Split into train, val, test (90%, 5%, 5%)\n",
    "    train_end = int(len(text) * 0.9)\n",
    "    val_end = int(len(text) * 0.95)\n",
    "    \n",
    "    return {\n",
    "        'train': text[:train_end],\n",
    "        'val': text[train_end:val_end],\n",
    "        'test': text[val_end:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0f4afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    # Normalize to standard Unicode form (avoids weird encodings)\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = [\n",
    "        unicodedata.normalize(\"NFKC\", line).strip()\n",
    "        for line in lines\n",
    "        if line.strip()  # Remove empty lines\n",
    "    ]\n",
    "    return \"\\n\".join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06fbe790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bpe_tokenizer(text, vocab_size=2000):\n",
    "    \"\"\"Create a BPE tokenizer with the specified vocabulary size\"\"\"\n",
    "    # Initialize a BPE tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    \n",
    "    # Configure pre-tokenization (how to split text into initial tokens)\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    \n",
    "    # Train the tokenizer\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\"])\n",
    "    tokenizer.train_from_iterator([text], trainer=trainer)\n",
    "    \n",
    "\n",
    "    tokenizer.save(\"bpe_tokenizer.json\")\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da0c8968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, tokenizer, seq_length=100):\n",
    "        self.seq_length = seq_length\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = tokenizer.encode(text)\n",
    "        self.tokens = encoding.ids\n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens with vocabulary size {self.vocab_size}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.tokens) - self.seq_length - 1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a sequence and the next token as target\"\"\"\n",
    "        # Input sequence\n",
    "        input_seq = self.tokens[idx:idx + self.seq_length]\n",
    "        # Target sequence (shifted by 1)\n",
    "        target_seq = self.tokens[idx + 1:idx + self.seq_length + 1]\n",
    "        \n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "587a9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, bidirectional=True):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim * self.num_directions, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden and cell states if not provided\n",
    "        if hidden is None:\n",
    "            h0 = torch.zeros(self.num_directions, batch_size, self.hidden_dim).to(device)\n",
    "            c0 = torch.zeros(self.num_directions, batch_size, self.hidden_dim).to(device)\n",
    "            hidden = (h0, c0)\n",
    "        \n",
    "        # Apply embedding\n",
    "        embed = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        output, hidden = self.lstm(embed, hidden)\n",
    "        \n",
    "        # Pass through linear layer\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros(self.num_directions, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_directions, batch_size, self.hidden_dim).to(device)\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f19cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, batch_size=32, num_epochs=5, learning_rate=0.001, clip_value=1.0, teacher_forcing_ratio=0.5):\n",
    "    \"\"\"Train the RNN model with validation and teacher forcing\"\"\"\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Use Adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_perplexities = []\n",
    "    val_perplexities = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            seq_length = inputs.size(1)\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            \n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Initialize output tensor\n",
    "            outputs = torch.zeros(batch_size, seq_length, model.vocab_size).to(device)\n",
    "            \n",
    "            # Teacher forcing with probability teacher_forcing_ratio\n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            if use_teacher_forcing:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = inputs\n",
    "                output, hidden = model(decoder_input, hidden)\n",
    "                outputs = output\n",
    "            else:\n",
    "                # Without teacher forcing: use own predictions as the next input\n",
    "                decoder_input = inputs[:, 0].unsqueeze(1)\n",
    "                for t in range(seq_length):\n",
    "                    output, hidden = model(decoder_input, hidden)\n",
    "                    outputs[:, t:t+1] = output[:, -1:, :]\n",
    "                    decoder_input = output[:, -1:, :].argmax(2)\n",
    "            \n",
    "            # Reshape outputs and targets for loss calculation\n",
    "            outputs = outputs.reshape(-1, outputs.shape[-1])\n",
    "            targets = targets.reshape(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Print progress (less frequently to reduce output)\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Calculate average training loss and perplexity\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_perplexity = np.exp(avg_train_loss)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_perplexities.append(train_perplexity)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # Initialize hidden state\n",
    "                hidden = model.init_hidden(inputs.size(0))\n",
    "                \n",
    "                outputs, _ = model(inputs, hidden)\n",
    "                outputs = outputs.reshape(-1, outputs.shape[-1])\n",
    "                targets = targets.reshape(-1)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Calculate average validation loss and perplexity\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_perplexity = np.exp(avg_val_loss)\n",
    "        \n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_perplexities.append(val_perplexity)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Perplexity: {train_perplexity:.4f}, '\n",
    "              f'Val Loss: {avg_val_loss:.4f}, Val Perplexity: {val_perplexity:.4f}')\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_perplexities, label='Train Perplexity')\n",
    "    plt.plot(val_perplexities, label='Val Perplexity')\n",
    "    plt.legend()\n",
    "    plt.title('Perplexity')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_perplexities': train_perplexities,\n",
    "        'val_perplexities': val_perplexities\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80a7ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seed_text='To be, or not to be', gen_length=100, temperature=0.8):\n",
    "    \"\"\"Generate text using the trained model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize seed text\n",
    "    tokens = tokenizer.encode(seed_text).ids\n",
    "    input_seq = torch.tensor([tokens], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    # Generate text\n",
    "    generated_tokens = tokens.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(gen_length):\n",
    "            # Get prediction for next token\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = output[:, -1, :] / temperature\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token = torch.multinomial(probabilities, 1).item()\n",
    "            \n",
    "            # Add to generated tokens\n",
    "            generated_tokens.append(next_token)\n",
    "            \n",
    "            # Update input for next iteration (use the last predicted token)\n",
    "            input_seq = torch.tensor([[next_token]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Decode generated tokens\n",
    "    generated_text = tokenizer.decode(generated_tokens)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2da7601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_beam_search(model, tokenizer, seed_text='To be, or not to be', gen_length=100, beam_width=5, temperature=0.8):\n",
    "    \"\"\"Generate text using beam search decoding\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize seed text\n",
    "    tokens = tokenizer.encode(seed_text).ids\n",
    "    input_seq = torch.tensor([tokens], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Initialize beam search\n",
    "    beams = [(input_seq, model.init_hidden(1), 0.0)]  # (sequence, hidden_state, log_prob)\n",
    "    finished_beams = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(gen_length):\n",
    "            candidates = []\n",
    "            \n",
    "            # Expand each beam\n",
    "            for seq, hidden, log_prob in beams:\n",
    "                # Get prediction for next token\n",
    "                output, new_hidden = model(seq[:, -1:], hidden)\n",
    "                logits = output[:, -1, :] / temperature\n",
    "                probs = F.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                # Get top-k candidates\n",
    "                top_probs, top_tokens = probs.topk(beam_width)\n",
    "                \n",
    "                for prob, token in zip(top_probs[0], top_tokens[0]):\n",
    "                    new_seq = torch.cat([seq, token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "                    new_log_prob = log_prob + prob.item()\n",
    "                    candidates.append((new_seq, new_hidden, new_log_prob))\n",
    "            \n",
    "            # Select top beams\n",
    "            candidates.sort(key=lambda x: x[2], reverse=True)\n",
    "            beams = candidates[:beam_width]\n",
    "        \n",
    "        # Select best sequence\n",
    "        best_seq = max(beams, key=lambda x: x[2])[0]\n",
    "        generated_tokens = best_seq[0].tolist()\n",
    "    \n",
    "    # Decode generated tokens\n",
    "    generated_text = tokenizer.decode(generated_tokens)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cea87d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perplexity(model, dataset, batch_size=32):\n",
    "    \"\"\"Evaluate model perplexity on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            \n",
    "            outputs, _ = model(inputs, hidden)\n",
    "            outputs = outputs.reshape(-1, outputs.shape[-1])\n",
    "            targets = targets.reshape(-1)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "            total_tokens += targets.size(0)\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c821a156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare data...\n",
      "Creating BPE tokenizer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n",
      "Loaded 307825 tokens with vocabulary size 1000\n",
      "Loaded 17797 tokens with vocabulary size 1000\n",
      "Loaded 18338 tokens with vocabulary size 1000\n",
      "Vocabulary size: 1000\n",
      "\n",
      "Training LSTM model...\n",
      "Epoch 1/1, Batch 50/4809, Loss: 6.9363\n",
      "Epoch 1/1, Batch 50/4809, Loss: 6.9363\n",
      "Epoch 1/1, Batch 100/4809, Loss: 6.1987\n",
      "Epoch 1/1, Batch 100/4809, Loss: 6.1987\n",
      "Epoch 1/1, Batch 150/4809, Loss: 5.9692\n",
      "Epoch 1/1, Batch 150/4809, Loss: 5.9692\n",
      "Epoch 1/1, Batch 200/4809, Loss: 5.9739\n",
      "Epoch 1/1, Batch 200/4809, Loss: 5.9739\n",
      "Epoch 1/1, Batch 250/4809, Loss: 0.2301\n",
      "Epoch 1/1, Batch 250/4809, Loss: 0.2301\n",
      "Epoch 1/1, Batch 300/4809, Loss: 6.0109\n",
      "Epoch 1/1, Batch 300/4809, Loss: 6.0109\n",
      "Epoch 1/1, Batch 350/4809, Loss: 5.9529\n",
      "Epoch 1/1, Batch 350/4809, Loss: 5.9529\n",
      "Epoch 1/1, Batch 400/4809, Loss: 5.9924\n",
      "Epoch 1/1, Batch 400/4809, Loss: 5.9924\n",
      "Epoch 1/1, Batch 450/4809, Loss: 5.9239\n",
      "Epoch 1/1, Batch 450/4809, Loss: 5.9239\n",
      "Epoch 1/1, Batch 500/4809, Loss: 0.1283\n",
      "Epoch 1/1, Batch 500/4809, Loss: 0.1283\n",
      "Epoch 1/1, Batch 550/4809, Loss: 5.9377\n",
      "Epoch 1/1, Batch 550/4809, Loss: 5.9377\n",
      "Epoch 1/1, Batch 600/4809, Loss: 0.0929\n",
      "Epoch 1/1, Batch 600/4809, Loss: 0.0929\n",
      "Epoch 1/1, Batch 650/4809, Loss: 0.0983\n",
      "Epoch 1/1, Batch 650/4809, Loss: 0.0983\n",
      "Epoch 1/1, Batch 700/4809, Loss: 0.1619\n",
      "Epoch 1/1, Batch 700/4809, Loss: 0.1619\n",
      "Epoch 1/1, Batch 750/4809, Loss: 5.9710\n",
      "Epoch 1/1, Batch 750/4809, Loss: 5.9710\n",
      "Epoch 1/1, Batch 800/4809, Loss: 0.0879\n",
      "Epoch 1/1, Batch 800/4809, Loss: 0.0879\n",
      "Epoch 1/1, Batch 850/4809, Loss: 0.1307\n",
      "Epoch 1/1, Batch 850/4809, Loss: 0.1307\n",
      "Epoch 1/1, Batch 900/4809, Loss: 5.9416\n",
      "Epoch 1/1, Batch 900/4809, Loss: 5.9416\n",
      "Epoch 1/1, Batch 950/4809, Loss: 5.9707\n",
      "Epoch 1/1, Batch 950/4809, Loss: 5.9707\n",
      "Epoch 1/1, Batch 1000/4809, Loss: 5.9674\n",
      "Epoch 1/1, Batch 1000/4809, Loss: 5.9674\n",
      "Epoch 1/1, Batch 1050/4809, Loss: 0.0814\n",
      "Epoch 1/1, Batch 1050/4809, Loss: 0.0814\n",
      "Epoch 1/1, Batch 1100/4809, Loss: 5.9356\n",
      "Epoch 1/1, Batch 1100/4809, Loss: 5.9356\n",
      "Epoch 1/1, Batch 1150/4809, Loss: 5.9359\n",
      "Epoch 1/1, Batch 1150/4809, Loss: 5.9359\n",
      "Epoch 1/1, Batch 1200/4809, Loss: 5.9236\n",
      "Epoch 1/1, Batch 1200/4809, Loss: 5.9236\n",
      "Epoch 1/1, Batch 1250/4809, Loss: 5.9831\n",
      "Epoch 1/1, Batch 1250/4809, Loss: 5.9831\n",
      "Epoch 1/1, Batch 1300/4809, Loss: 5.9706\n",
      "Epoch 1/1, Batch 1300/4809, Loss: 5.9706\n",
      "Epoch 1/1, Batch 1350/4809, Loss: 0.0768\n",
      "Epoch 1/1, Batch 1350/4809, Loss: 0.0768\n",
      "Epoch 1/1, Batch 1400/4809, Loss: 0.0763\n",
      "Epoch 1/1, Batch 1400/4809, Loss: 0.0763\n",
      "Epoch 1/1, Batch 1450/4809, Loss: 0.0640\n",
      "Epoch 1/1, Batch 1450/4809, Loss: 0.0640\n",
      "Epoch 1/1, Batch 1500/4809, Loss: 0.0723\n",
      "Epoch 1/1, Batch 1500/4809, Loss: 0.0723\n",
      "Epoch 1/1, Batch 1550/4809, Loss: 0.0663\n",
      "Epoch 1/1, Batch 1550/4809, Loss: 0.0663\n",
      "Epoch 1/1, Batch 1600/4809, Loss: 5.9041\n",
      "Epoch 1/1, Batch 1600/4809, Loss: 5.9041\n",
      "Epoch 1/1, Batch 1650/4809, Loss: 0.0709\n",
      "Epoch 1/1, Batch 1650/4809, Loss: 0.0709\n",
      "Epoch 1/1, Batch 1700/4809, Loss: 5.9319\n",
      "Epoch 1/1, Batch 1700/4809, Loss: 5.9319\n",
      "Epoch 1/1, Batch 1750/4809, Loss: 0.0699\n",
      "Epoch 1/1, Batch 1750/4809, Loss: 0.0699\n",
      "Epoch 1/1, Batch 1800/4809, Loss: 5.9175\n",
      "Epoch 1/1, Batch 1800/4809, Loss: 5.9175\n",
      "Epoch 1/1, Batch 1850/4809, Loss: 5.9209\n",
      "Epoch 1/1, Batch 1850/4809, Loss: 5.9209\n",
      "Epoch 1/1, Batch 1900/4809, Loss: 0.0747\n",
      "Epoch 1/1, Batch 1900/4809, Loss: 0.0747\n",
      "Epoch 1/1, Batch 1950/4809, Loss: 5.9391\n",
      "Epoch 1/1, Batch 1950/4809, Loss: 5.9391\n",
      "Epoch 1/1, Batch 2000/4809, Loss: 0.0644\n",
      "Epoch 1/1, Batch 2000/4809, Loss: 0.0644\n",
      "Epoch 1/1, Batch 2050/4809, Loss: 0.0704\n",
      "Epoch 1/1, Batch 2050/4809, Loss: 0.0704\n",
      "Epoch 1/1, Batch 2100/4809, Loss: 0.0618\n",
      "Epoch 1/1, Batch 2150/4809, Loss: 0.0660\n",
      "Epoch 1/1, Batch 2200/4809, Loss: 6.0061\n",
      "Epoch 1/1, Batch 2250/4809, Loss: 5.9787\n",
      "Epoch 1/1, Batch 2300/4809, Loss: 5.9730\n",
      "Epoch 1/1, Batch 2350/4809, Loss: 0.0891\n",
      "Epoch 1/1, Batch 2400/4809, Loss: 5.9443\n",
      "Epoch 1/1, Batch 2450/4809, Loss: 6.0677\n",
      "Epoch 1/1, Batch 2500/4809, Loss: 5.9291\n",
      "Epoch 1/1, Batch 2550/4809, Loss: 5.9950\n",
      "Epoch 1/1, Batch 2600/4809, Loss: 5.9452\n",
      "Epoch 1/1, Batch 2650/4809, Loss: 5.9282\n",
      "Epoch 1/1, Batch 2700/4809, Loss: 0.1104\n",
      "Epoch 1/1, Batch 2750/4809, Loss: 0.0780\n",
      "Epoch 1/1, Batch 2800/4809, Loss: 0.0759\n",
      "Epoch 1/1, Batch 2850/4809, Loss: 5.9031\n",
      "Epoch 1/1, Batch 2900/4809, Loss: 0.0824\n",
      "Epoch 1/1, Batch 2950/4809, Loss: 5.8947\n",
      "Epoch 1/1, Batch 3000/4809, Loss: 0.0619\n",
      "Epoch 1/1, Batch 3050/4809, Loss: 5.9174\n",
      "Epoch 1/1, Batch 3100/4809, Loss: 5.9125\n",
      "Epoch 1/1, Batch 3150/4809, Loss: 5.9203\n",
      "Epoch 1/1, Batch 3200/4809, Loss: 5.9476\n",
      "Epoch 1/1, Batch 3250/4809, Loss: 5.9209\n",
      "Epoch 1/1, Batch 3300/4809, Loss: 0.0548\n",
      "Epoch 1/1, Batch 3350/4809, Loss: 0.0572\n",
      "Epoch 1/1, Batch 3400/4809, Loss: 5.9338\n",
      "Epoch 1/1, Batch 3450/4809, Loss: 5.9332\n",
      "Epoch 1/1, Batch 3500/4809, Loss: 0.0653\n",
      "Epoch 1/1, Batch 3550/4809, Loss: 5.9517\n",
      "Epoch 1/1, Batch 3600/4809, Loss: 0.0624\n",
      "Epoch 1/1, Batch 3650/4809, Loss: 5.9682\n",
      "Epoch 1/1, Batch 3700/4809, Loss: 0.0568\n",
      "Epoch 1/1, Batch 3750/4809, Loss: 0.0742\n",
      "Epoch 1/1, Batch 3800/4809, Loss: 5.9013\n",
      "Epoch 1/1, Batch 3850/4809, Loss: 5.9452\n",
      "Epoch 1/1, Batch 3900/4809, Loss: 0.0535\n",
      "Epoch 1/1, Batch 3950/4809, Loss: 5.9566\n",
      "Epoch 1/1, Batch 4000/4809, Loss: 5.9219\n",
      "Epoch 1/1, Batch 4050/4809, Loss: 5.9156\n",
      "Epoch 1/1, Batch 4100/4809, Loss: 0.0565\n",
      "Epoch 1/1, Batch 4150/4809, Loss: 0.0621\n",
      "Epoch 1/1, Batch 4200/4809, Loss: 0.0643\n",
      "Epoch 1/1, Batch 4250/4809, Loss: 0.0605\n",
      "Epoch 1/1, Batch 4300/4809, Loss: 5.9156\n",
      "Epoch 1/1, Batch 4350/4809, Loss: 5.9056\n",
      "Epoch 1/1, Batch 4400/4809, Loss: 0.0568\n",
      "Epoch 1/1, Batch 4450/4809, Loss: 5.9285\n",
      "Epoch 1/1, Batch 4500/4809, Loss: 0.0893\n",
      "Epoch 1/1, Batch 4550/4809, Loss: 0.0750\n",
      "Epoch 1/1, Batch 4600/4809, Loss: 5.9242\n",
      "Epoch 1/1, Batch 4650/4809, Loss: 5.9746\n",
      "Epoch 1/1, Batch 4700/4809, Loss: 0.0668\n",
      "Epoch 1/1, Batch 4750/4809, Loss: 5.9184\n",
      "Epoch 1/1, Batch 4800/4809, Loss: 0.0893\n",
      "Epoch 1/1, Train Loss: 3.0542, Train Perplexity: 21.2043, Val Loss: 0.1058, Val Perplexity: 1.1116\n",
      "\n",
      "Evaluating models...\n",
      "LSTM Test Perplexity: 1.1362\n",
      "\n",
      "Generating text with LSTM model:\n",
      "To be , or not to be , To such this H a er S my w my part a , to th ir es ; ed , ad , ad , do not ; - ise must ise must ise must be must be for de l un you end you should I say I say ' tis ' tis ' Nay , er : pray ss er eye er eye be it if it if it af it af will ac li er his - heart . C a row l self by thee ! an our h ood S ce , ra d you\n",
      "\n",
      "Generating text with LSTM model using beam search:\n",
      "To be , or not to be a d a d a a : s , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading Shakespeare data...\")\n",
    "data = load_tiny_shakespeare()\n",
    "\n",
    "# Create BPE tokenizer (smaller vocab size for faster training)\n",
    "print(\"Creating BPE tokenizer...\")\n",
    "tokenizer = create_bpe_tokenizer(data['train'], vocab_size=1000)\n",
    "\n",
    "# Create datasets\n",
    "print(\"Preparing datasets...\")\n",
    "seq_length = 100  # Using fixed sequence length for simplicity\n",
    "train_dataset = TextDataset(data['train'], tokenizer, seq_length)\n",
    "val_dataset = TextDataset(data['val'], tokenizer, seq_length)\n",
    "test_dataset = TextDataset(data['test'], tokenizer, seq_length)\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create models (with smaller sizes for lightweight training)\n",
    "# rnn_model = VanillaRNN(vocab_size, embedding_dim=64, hidden_dim=128, bidirectional=True)\n",
    "lstm_model = SimpleLSTM(vocab_size, embedding_dim=64, hidden_dim=128, bidirectional=True)\n",
    "\n",
    "# Train models (reduced epochs and batch size for faster training)\n",
    "# print(\"\\nTraining Vanilla RNN model...\")\n",
    "# rnn_results = train_model(rnn_model, train_dataset, val_dataset, \n",
    "#                          batch_size=16, num_epochs=3, learning_rate=0.001)\n",
    "\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "lstm_results = train_model(lstm_model, train_dataset, val_dataset, \n",
    "                        batch_size=64,  # Increased from 16 to 64\n",
    "                        num_epochs=1, \n",
    "                        learning_rate=0.01,\n",
    "                        teacher_forcing_ratio=0.5)\n",
    "\n",
    "# Evaluate models on test set\n",
    "print(\"\\nEvaluating models...\")\n",
    "# rnn_perplexity = evaluate_perplexity(rnn_model, test_dataset)\n",
    "lstm_perplexity = evaluate_perplexity(lstm_model, test_dataset)\n",
    "\n",
    "# print(f\"Vanilla RNN Test Perplexity: {rnn_perplexity:.4f}\")\n",
    "print(f\"LSTM Test Perplexity: {lstm_perplexity:.4f}\")\n",
    "\n",
    "# Compare models\n",
    "model_results = {\n",
    "# 'Vanilla RNN': rnn_results,\n",
    "'LSTM': lstm_results\n",
    "}\n",
    "# compare_models(model_results)\n",
    "\n",
    "# Generate text samples\n",
    "# print(\"\\nGenerating text with RNN model:\")\n",
    "# rnn_text = generate_text(rnn_model, tokenizer, seed_text=\"To be, or not to be\", gen_length=100)\n",
    "# print(rnn_text)\n",
    "\n",
    "print(\"\\nGenerating text with LSTM model:\")\n",
    "lstm_text = generate_text(lstm_model, tokenizer, seed_text=\"To be, or not to be\", gen_length=100)\n",
    "print(lstm_text)\n",
    "\n",
    "# Generate text using beam search\n",
    "print(\"\\nGenerating text with LSTM model using beam search:\")\n",
    "lstm_text_beam = generate_text_beam_search(lstm_model, tokenizer, \n",
    "                                          seed_text=\"To be, or not to be\", \n",
    "                                          gen_length=100, \n",
    "                                          beam_width=5)\n",
    "print(lstm_text_beam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb663b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the LSTM model\n",
    "torch.save(lstm_model.state_dict(), 'lstm_model_codev4.0.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
