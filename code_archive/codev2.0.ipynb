{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, GRU, LSTM, SimpleRNN, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import tokenizers\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from keras import backend as K\n",
    "import heapq\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "class ShakespeareTextGeneration:\n",
    "    def __init__(self, file_path=\"shakespeare.txt\", seq_length=40, batch_size=64):\n",
    "        \"\"\"\n",
    "        Initialize the text generation class\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the Shakespeare dataset\n",
    "            seq_length: Length of input sequences\n",
    "            batch_size: Batch size for training\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.raw_text = None\n",
    "        self.tokenizer = None\n",
    "        self.total_words = None\n",
    "        self.input_sequences = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.model = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load Shakespeare text data\"\"\"\n",
    "        try:\n",
    "            with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "                self.raw_text = file.read()\n",
    "                print(f\"Data loaded successfully. Total characters: {len(self.raw_text)}\")\n",
    "                print(f\"First 500 characters: {self.raw_text[:500]}...\")\n",
    "                return True\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {self.file_path}\")\n",
    "            # Download Shakespeare data if not available\n",
    "            import requests\n",
    "            print(\"Attempting to download Shakespeare text...\")\n",
    "            url = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open('shakespeare.txt', 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                print(\"Shakespeare text downloaded successfully.\")\n",
    "                self.file_path = 'shakespeare.txt'\n",
    "                with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "                    self.raw_text = file.read()\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Failed to download Shakespeare text.\")\n",
    "                return False\n",
    "    \n",
    "    def preprocess_text(self):\n",
    "        \"\"\"Basic text preprocessing\"\"\"\n",
    "        # Remove special characters and numbers\n",
    "        self.raw_text = re.sub(r'[^\\w\\s]', ' ', self.raw_text)\n",
    "        self.raw_text = re.sub(r'\\d+', ' ', self.raw_text)\n",
    "        # Convert to lowercase\n",
    "        self.raw_text = self.raw_text.lower()\n",
    "        # Remove extra whitespaces\n",
    "        self.raw_text = re.sub(r'\\s+', ' ', self.raw_text).strip()\n",
    "        print(\"Basic preprocessing complete.\")\n",
    "        print(f\"Preprocessed sample: {self.raw_text[:500]}...\")\n",
    "        return self.raw_text\n",
    "    \n",
    "    def tokenize_wordpiece(self, vocab_size=10000):\n",
    "        \"\"\"Tokenize text using WordPiece/BPE approach\"\"\"\n",
    "        # Initialize BPE tokenizer\n",
    "        self.bpe_tokenizer = ByteLevelBPETokenizer()\n",
    "        \n",
    "        # Save text to temporary file for tokenizer training\n",
    "        with open(\"temp_shakespeare.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(self.raw_text)\n",
    "        \n",
    "        # Train the tokenizer\n",
    "        self.bpe_tokenizer.train(\n",
    "            files=[\"temp_shakespeare.txt\"],\n",
    "            vocab_size=vocab_size,\n",
    "            min_frequency=2,\n",
    "            special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "        )\n",
    "        \n",
    "        # Save the tokenizer\n",
    "        os.makedirs(\"shakespeare_tokenizer\", exist_ok=True)\n",
    "        self.bpe_tokenizer.save_model(\"shakespeare_tokenizer\")\n",
    "        \n",
    "        # Encode the text\n",
    "        encoded = self.bpe_tokenizer.encode(self.raw_text)\n",
    "        self.bpe_tokens = encoded.ids\n",
    "        self.bpe_vocab_size = self.bpe_tokenizer.get_vocab_size()\n",
    "        \n",
    "        print(f\"BPE Tokenization complete. Vocabulary size: {self.bpe_vocab_size}\")\n",
    "        print(f\"Sample encoded tokens: {self.bpe_tokens[:20]}\")\n",
    "        \n",
    "        # Clean up temp file\n",
    "        os.remove(\"temp_shakespeare.txt\")\n",
    "        \n",
    "        return self.bpe_tokens\n",
    "    \n",
    "    def tokenize_words(self, vocab_size=10000):\n",
    "        \"\"\"Simple word-level tokenization\"\"\"\n",
    "        # Split text into words\n",
    "        words = self.raw_text.split()\n",
    "        print(f\"Total words in corpus: {len(words)}\")\n",
    "        \n",
    "        # Create a word-level tokenizer\n",
    "        self.tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "        self.tokenizer.fit_on_texts([self.raw_text])\n",
    "        self.total_words = min(vocab_size, len(self.tokenizer.word_index) + 1)\n",
    "        print(f\"Vocabulary size: {self.total_words}\")\n",
    "        \n",
    "        # Convert text to sequence of tokens\n",
    "        sequences = self.tokenizer.texts_to_sequences([self.raw_text])[0]\n",
    "        \n",
    "        # Create input sequences for training\n",
    "        self.input_sequences = []\n",
    "        for i in range(1, len(sequences) - self.seq_length):\n",
    "            self.input_sequences.append(sequences[i:i+self.seq_length+1])\n",
    "        \n",
    "        print(f\"Number of training sequences: {len(self.input_sequences)}\")\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        self.input_sequences = np.array(self.input_sequences)\n",
    "        \n",
    "        # Split into training inputs and outputs\n",
    "        self.X = self.input_sequences[:, :-1]\n",
    "        self.y = self.input_sequences[:, -1]\n",
    "        \n",
    "        # One-hot encode the outputs\n",
    "        self.y = tf.keras.utils.to_categorical(self.y, num_classes=self.total_words)\n",
    "        \n",
    "        return self.X, self.y\n",
    "    \n",
    "    def generate_sequences_varying_lengths(self, min_length=10, max_length=100, num_samples=1000):\n",
    "        \"\"\"Generate sequences of varying lengths to test model robustness\"\"\"\n",
    "        sequences = self.tokenizer.texts_to_sequences([self.raw_text])[0]\n",
    "        varying_sequences = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            seq_len = np.random.randint(min_length, max_length)\n",
    "            start_idx = np.random.randint(0, len(sequences) - seq_len - 1)\n",
    "            varying_sequences.append(sequences[start_idx:start_idx+seq_len+1])\n",
    "        \n",
    "        # Pad sequences\n",
    "        padded_sequences = pad_sequences(varying_sequences, maxlen=max_length, padding='pre')\n",
    "        \n",
    "        # Split into inputs and outputs\n",
    "        X_varying = padded_sequences[:, :-1]\n",
    "        y_varying = padded_sequences[:, -1]\n",
    "        \n",
    "        # One-hot encode outputs\n",
    "        y_varying = tf.keras.utils.to_categorical(y_varying, num_classes=self.total_words)\n",
    "        \n",
    "        print(f\"Generated {len(varying_sequences)} sequences of varying lengths\")\n",
    "        return X_varying, y_varying\n",
    "    \n",
    "    def build_vanilla_rnn(self, embedding_dim=100, rnn_units=256, bidirectional=False):\n",
    "        \"\"\"Build a vanilla RNN model\"\"\"\n",
    "        inputs = Input(shape=(self.seq_length,))\n",
    "        embedding = Embedding(input_dim=self.total_words, output_dim=embedding_dim)(inputs)\n",
    "        \n",
    "        if bidirectional:\n",
    "            rnn_layer = Bidirectional(SimpleRNN(rnn_units, return_sequences=True))(embedding)\n",
    "            rnn_layer = Bidirectional(SimpleRNN(rnn_units))(rnn_layer)\n",
    "        else:\n",
    "            rnn_layer = SimpleRNN(rnn_units, return_sequences=True)(embedding)\n",
    "            rnn_layer = SimpleRNN(rnn_units)(rnn_layer)\n",
    "        \n",
    "        outputs = Dense(self.total_words, activation='softmax')(rnn_layer)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        print(\"Vanilla RNN model built:\")\n",
    "        print(model.summary())\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_lstm_model(self, embedding_dim=100, lstm_units=256, bidirectional=False):\n",
    "        \"\"\"Build an LSTM model\"\"\"\n",
    "        inputs = Input(shape=(self.seq_length,))\n",
    "        embedding = Embedding(input_dim=self.total_words, output_dim=embedding_dim)(inputs)\n",
    "        \n",
    "        if bidirectional:\n",
    "            lstm_layer = Bidirectional(LSTM(lstm_units, return_sequences=True))(embedding)\n",
    "            lstm_layer = Bidirectional(LSTM(lstm_units))(lstm_layer)\n",
    "        else:\n",
    "            lstm_layer = LSTM(lstm_units, return_sequences=True)(embedding)\n",
    "            lstm_layer = LSTM(lstm_units)(lstm_layer)\n",
    "        \n",
    "        outputs = Dense(self.total_words, activation='softmax')(lstm_layer)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        print(\"LSTM model built:\")\n",
    "        print(model.summary())\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_gru_model(self, embedding_dim=100, gru_units=256, bidirectional=False):\n",
    "        \"\"\"Build a GRU model\"\"\"\n",
    "        inputs = Input(shape=(self.seq_length,))\n",
    "        embedding = Embedding(input_dim=self.total_words, output_dim=embedding_dim)(inputs)\n",
    "        \n",
    "        if bidirectional:\n",
    "            gru_layer = Bidirectional(GRU(gru_units, return_sequences=True))(embedding)\n",
    "            gru_layer = Bidirectional(GRU(gru_units))(gru_layer)\n",
    "        else:\n",
    "            gru_layer = GRU(gru_units, return_sequences=True)(embedding)\n",
    "            gru_layer = GRU(gru_units)(gru_layer)\n",
    "        \n",
    "        outputs = Dense(self.total_words, activation='softmax')(gru_layer)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        print(\"GRU model built:\")\n",
    "        print(model.summary())\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, model, optimizer_name='adam', learning_rate=0.001, epochs=50, \n",
    "                   gradient_clipping=True, clip_value=1.0, teacher_forcing=True, \n",
    "                   teacher_forcing_ratio=0.5):\n",
    "        \"\"\"Train the model with various optimization strategies\"\"\"\n",
    "        # Optimizer selection\n",
    "        if optimizer_name.lower() == 'adam':\n",
    "            if gradient_clipping:\n",
    "                optimizer = Adam(learning_rate=learning_rate, clipvalue=clip_value)\n",
    "            else:\n",
    "                optimizer = Adam(learning_rate=learning_rate)\n",
    "        elif optimizer_name.lower() == 'rmsprop':\n",
    "            if gradient_clipping:\n",
    "                optimizer = RMSprop(learning_rate=learning_rate, clipvalue=clip_value)\n",
    "            else:\n",
    "                optimizer = RMSprop(learning_rate=learning_rate)\n",
    "        else:\n",
    "            print(f\"Unknown optimizer: {optimizer_name}. Using Adam instead.\")\n",
    "            optimizer = Adam(learning_rate=learning_rate)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        # Callbacks\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            filepath=f'shakespeare_{model.name}_checkpoint.h5',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max'\n",
    "        )\n",
    "        \n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Teacher forcing implementation (simplified for demonstration)\n",
    "        if teacher_forcing:\n",
    "            # Split the data for teacher forcing\n",
    "            train_size = int(0.8 * len(self.X))\n",
    "            X_train, X_val = self.X[:train_size], self.X[train_size:]\n",
    "            y_train, y_val = self.y[:train_size], self.y[train_size:]\n",
    "            \n",
    "            # Teacher forcing training\n",
    "            for epoch in range(epochs):\n",
    "                print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "                \n",
    "                # With teacher forcing\n",
    "                if np.random.random() < teacher_forcing_ratio:\n",
    "                    model.fit(\n",
    "                        X_train, y_train,\n",
    "                        batch_size=self.batch_size,\n",
    "                        epochs=1,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[model_checkpoint],\n",
    "                        verbose=1\n",
    "                    )\n",
    "                \n",
    "                # Without teacher forcing - using model's predictions\n",
    "                else:\n",
    "                    # Make predictions\n",
    "                    pred_sequences = []\n",
    "                    for i in range(0, len(X_train), self.batch_size):\n",
    "                        batch_X = X_train[i:i+self.batch_size]\n",
    "                        # Get model predictions for all but last token\n",
    "                        for j in range(self.seq_length-1):\n",
    "                            preds = model.predict(batch_X[:, :j+1], verbose=0)\n",
    "                            next_token = np.argmax(preds, axis=1)\n",
    "                            # Set the next token in the sequence\n",
    "                            if j+1 < self.seq_length:\n",
    "                                for k in range(len(batch_X)):\n",
    "                                    batch_X[k, j+1] = next_token[k]\n",
    "                        \n",
    "                        pred_sequences.append(batch_X)\n",
    "                    \n",
    "                    # Combine predictions\n",
    "                    pred_X = np.vstack(pred_sequences) if pred_sequences else X_train\n",
    "                    \n",
    "                    # Train on these sequences\n",
    "                    model.fit(\n",
    "                        pred_X, y_train[:len(pred_X)],\n",
    "                        batch_size=self.batch_size,\n",
    "                        epochs=1,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[model_checkpoint],\n",
    "                        verbose=1\n",
    "                    )\n",
    "            \n",
    "            # Final evaluation\n",
    "            val_loss, val_acc = model.evaluate(X_val, y_val, verbose=1)\n",
    "            print(f\"Final validation loss: {val_loss}, accuracy: {val_acc}\")\n",
    "            \n",
    "        else:\n",
    "            # Regular training without teacher forcing\n",
    "            history = model.fit(\n",
    "                self.X, self.y,\n",
    "                batch_size=self.batch_size,\n",
    "                epochs=epochs,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[model_checkpoint, early_stopping],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Plot training history\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history.history['loss'], label='Train Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.title('Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "            plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "            plt.title('Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'training_history_{model.name}.png')\n",
    "            plt.show()\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def temperature_sampling(self, preds, temperature=1.0):\n",
    "        \"\"\"Sample with temperature control to adjust randomness\"\"\"\n",
    "        if temperature == 0:  # Deterministic (greedy) sampling\n",
    "            return np.argmax(preds)\n",
    "        \n",
    "        # Scale predictions by temperature\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        return np.argmax(probas)\n",
    "    \n",
    "    def beam_search(self, seed_text, beam_width=3, max_length=50):\n",
    "        \"\"\"Implement beam search to generate higher-quality text\"\"\"\n",
    "        # Tokenize seed text\n",
    "        seed_seq = self.tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        \n",
    "        # Pad sequence to required length\n",
    "        padded_seq = pad_sequences([seed_seq], maxlen=self.seq_length, padding='pre')\n",
    "        \n",
    "        # Initial beam\n",
    "        beams = [(0, padded_seq, [])]  # (score, sequence, generated_words)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            new_beams = []\n",
    "            for score, seq, words in beams:\n",
    "                # Get model predictions\n",
    "                preds = self.model.predict(seq, verbose=0)[0]\n",
    "                \n",
    "                # Get top k predictions\n",
    "                top_indices = np.argsort(preds)[-beam_width:]\n",
    "                \n",
    "                for idx in top_indices:\n",
    "                    word = self.tokenizer.index_word.get(idx, \"<OOV>\")\n",
    "                    new_score = score + np.log(preds[idx])\n",
    "                    \n",
    "                    # Create new sequence by shifting and adding new token\n",
    "                    new_seq = np.copy(seq)\n",
    "                    new_seq[0] = np.append(new_seq[0][1:], [idx])\n",
    "                    \n",
    "                    new_beams.append((new_score, new_seq, words + [word]))\n",
    "            \n",
    "            # Keep only the best beams\n",
    "            beams = heapq.nlargest(beam_width, new_beams, key=lambda x: x[0])\n",
    "        \n",
    "        # Return the best sequence\n",
    "        best_score, _, best_words = beams[0]\n",
    "        return ' '.join(best_words)\n",
    "    \n",
    "    def generate_text(self, seed_text, max_length=100, temperature=1.0, use_beam_search=False, beam_width=3):\n",
    "        \"\"\"Generate text with temperature control or beam search\"\"\"\n",
    "        if use_beam_search:\n",
    "            return self.beam_search(seed_text, beam_width=beam_width, max_length=max_length)\n",
    "        \n",
    "        # Tokenize the seed text\n",
    "        seed_seq = self.tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        generated_text = seed_text\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Pad the sequence\n",
    "            padded_seq = pad_sequences([seed_seq], maxlen=self.seq_length, padding='pre')\n",
    "            \n",
    "            # Get model predictions\n",
    "            preds = self.model.predict(padded_seq, verbose=0)[0]\n",
    "            \n",
    "            # Sample with temperature\n",
    "            next_index = self.temperature_sampling(preds, temperature)\n",
    "            \n",
    "            # Convert to word\n",
    "            next_word = self.tokenizer.index_word.get(next_index, \"<OOV>\")\n",
    "            \n",
    "            # Add to generated text\n",
    "            generated_text += \" \" + next_word\n",
    "            \n",
    "            # Update seed sequence\n",
    "            seed_seq.append(next_index)\n",
    "            seed_seq = seed_seq[1:]\n",
    "        \n",
    "        return generated_text\n",
    "    \n",
    "    def calculate_perplexity(self, text):\n",
    "        \"\"\"Calculate perplexity as an evaluation metric\"\"\"\n",
    "        sequences = self.tokenizer.texts_to_sequences([text])[0]\n",
    "        \n",
    "        # Prepare sequences for evaluation\n",
    "        X_eval = []\n",
    "        y_eval = []\n",
    "        \n",
    "        for i in range(len(sequences) - self.seq_length):\n",
    "            X_eval.append(sequences[i:i+self.seq_length])\n",
    "            y_eval.append(sequences[i+self.seq_length])\n",
    "        \n",
    "        X_eval = np.array(X_eval)\n",
    "        y_eval = tf.keras.utils.to_categorical(y_eval, num_classes=self.total_words)\n",
    "        \n",
    "        # Calculate loss (cross-entropy)\n",
    "        loss = self.model.evaluate(X_eval, y_eval, verbose=0)\n",
    "        \n",
    "        # Perplexity = exp(cross-entropy loss)\n",
    "        perplexity = np.exp(loss)\n",
    "        return perplexity\n",
    "    \n",
    "    def compare_models(self, models_dict, seed_text, max_length=100, temperatures=[0.2, 0.5, 1.0]):\n",
    "        \"\"\"Compare text generated by different models\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for model_name, model in models_dict.items():\n",
    "            self.model = model  # Set current model\n",
    "            model_results = {}\n",
    "            \n",
    "            for temp in temperatures:\n",
    "                generated_text = self.generate_text(\n",
    "                    seed_text=seed_text,\n",
    "                    max_length=max_length,\n",
    "                    temperature=temp\n",
    "                )\n",
    "                \n",
    "                # Calculate perplexity\n",
    "                perplexity = self.calculate_perplexity(generated_text)\n",
    "                \n",
    "                model_results[f\"temp_{temp}\"] = {\n",
    "                    \"text\": generated_text,\n",
    "                    \"perplexity\": perplexity\n",
    "                }\n",
    "            \n",
    "            # Generate with beam search\n",
    "            beam_text = self.generate_text(\n",
    "                seed_text=seed_text,\n",
    "                max_length=max_length,\n",
    "                use_beam_search=True,\n",
    "                beam_width=3\n",
    "            )\n",
    "            \n",
    "            beam_perplexity = self.calculate_perplexity(beam_text)\n",
    "            model_results[\"beam_search\"] = {\n",
    "                \"text\": beam_text,\n",
    "                \"perplexity\": beam_perplexity\n",
    "            }\n",
    "            \n",
    "            results[model_name] = model_results\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_style(self, original_text, generated_text):\n",
    "        \"\"\"Analyze stylistic differences between original and generated text\"\"\"\n",
    "        # Tokenize both texts\n",
    "        original_words = original_text.split()\n",
    "        generated_words = generated_text.split()\n",
    "        \n",
    "        # Calculate basic statistics\n",
    "        avg_word_len_original = np.mean([len(w) for w in original_words])\n",
    "        avg_word_len_generated = np.mean([len(w) for w in generated_words])\n",
    "        \n",
    "        # Vocabulary diversity (unique words / total words)\n",
    "        vocab_diversity_original = len(set(original_words)) / len(original_words)\n",
    "        vocab_diversity_generated = len(set(generated_words)) / len(generated_words)\n",
    "        \n",
    "        # Sentence length (approximate by splitting on periods)\n",
    "        original_sentences = original_text.split('.')\n",
    "        generated_sentences = generated_text.split('.')\n",
    "        \n",
    "        avg_sent_len_original = np.mean([len(s.split()) for s in original_sentences if s])\n",
    "        avg_sent_len_generated = np.mean([len(s.split()) for s in generated_sentences if s])\n",
    "        \n",
    "        # Word frequency distribution\n",
    "        from collections import Counter\n",
    "        original_freq = Counter(original_words)\n",
    "        generated_freq = Counter(generated_words)\n",
    "        \n",
    "        # Most common words\n",
    "        original_common = original_freq.most_common(10)\n",
    "        generated_common = generated_freq.most_common(10)\n",
    "        \n",
    "        # Calculate overlap in most common words\n",
    "        original_common_words = set([w for w, _ in original_common])\n",
    "        generated_common_words = set([w for w, _ in generated_common])\n",
    "        common_overlap = len(original_common_words.intersection(generated_common_words))\n",
    "        \n",
    "        results = {\n",
    "            \"avg_word_length\": {\n",
    "                \"original\": avg_word_len_original,\n",
    "                \"generated\": avg_word_len_generated,\n",
    "                \"difference\": avg_word_len_original - avg_word_len_generated\n",
    "            },\n",
    "            \"vocabulary_diversity\": {\n",
    "                \"original\": vocab_diversity_original,\n",
    "                \"generated\": vocab_diversity_generated,\n",
    "                \"difference\": vocab_diversity_original - vocab_diversity_generated\n",
    "            },\n",
    "            \"avg_sentence_length\": {\n",
    "                \"original\": avg_sent_len_original,\n",
    "                \"generated\": avg_sent_len_generated,\n",
    "                \"difference\": avg_sent_len_original - avg_sent_len_generated\n",
    "            },\n",
    "            \"common_words\": {\n",
    "                \"original\": original_common,\n",
    "                \"generated\": generated_common,\n",
    "                \"overlap\": common_overlap,\n",
    "                \"overlap_percentage\": common_overlap / 10 * 100\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Run the entire pipeline\n",
    "def main():\n",
    "    # Initialize\n",
    "    shakespeare = ShakespeareTextGeneration(file_path=\"shakespeare.txt\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    if not shakespeare.load_data():\n",
    "        print(\"Failed to load data. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    shakespeare.preprocess_text()\n",
    "    \n",
    "    # Tokenize text (choose one method)\n",
    "    # shakespeare.tokenize_wordpiece()  # BPE tokenization\n",
    "    shakespeare.tokenize_words()  # Word-level tokenization\n",
    "    \n",
    "    # Generate sequences of varying lengths to test robustness\n",
    "    X_varying, y_varying = shakespeare.generate_sequences_varying_lengths()\n",
    "    \n",
    "    # Build models\n",
    "    vanilla_rnn = shakespeare.build_vanilla_rnn(bidirectional=True)\n",
    "    lstm_model = shakespeare.build_lstm_model(bidirectional=True)\n",
    "    gru_model = shakespeare.build_gru_model(bidirectional=True)\n",
    "    \n",
    "    # Train models (usually you'd train one at a time)\n",
    "    # Choose which model to train\n",
    "    model_to_train = lstm_model  # Change to train different models\n",
    "    \n",
    "    trained_model = shakespeare.train_model(\n",
    "        model=model_to_train,\n",
    "        optimizer_name='adam',\n",
    "        learning_rate=0.001,\n",
    "        epochs=20,  # Reduced for demonstration\n",
    "        gradient_clipping=True,\n",
    "        clip_value=1.0,\n",
    "        teacher_forcing=True,\n",
    "        teacher_forcing_ratio=0.5\n",
    "    )\n",
    "    \n",
    "    # Generate text with different temperatures\n",
    "    seed_text = \"to be or not to be\"\n",
    "    \n",
    "    print(\"\\n--- Temperature = 0.2 (more focused) ---\")\n",
    "    generated_text_low_temp = shakespeare.generate_text(\n",
    "        seed_text=seed_text,\n",
    "        temperature=0.2\n",
    "    )\n",
    "    print(generated_text_low_temp)\n",
    "    \n",
    "    print(\"\\n--- Temperature = 1.0 (balanced) ---\")\n",
    "    generated_text_med_temp = shakespeare.generate_text(\n",
    "        seed_text=seed_text,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    print(generated_text_med_temp)\n",
    "    \n",
    "    print(\"\\n--- Temperature = 1.5 (more random) ---\")\n",
    "    generated_text_high_temp = shakespeare.generate_text(\n",
    "        seed_text=seed_text,\n",
    "        temperature=1.5\n",
    "    )\n",
    "    print(generated_text_high_temp)\n",
    "    \n",
    "    print(\"\\n--- Beam Search ---\")\n",
    "    generated_text_beam = shakespeare.generate_text(\n",
    "        seed_text=seed_text,\n",
    "        use_beam_search=True,\n",
    "        beam_width=3\n",
    "    )\n",
    "    print(generated_text_beam)\n",
    "    \n",
    "    # Sample from original text for style comparison\n",
    "    original_sample = shakespeare.raw_text[1000:3000]  # Sample from Shakespeare's text\n",
    "    \n",
    "    # Analyze and compare styles\n",
    "    style_analysis = shakespeare.analyze_style(original_sample, generated_text_med_temp)\n",
    "    \n",
    "    print(\"\\n--- Style Analysis ---\")\n",
    "    print(f\"Average Word Length: Original={style_analysis['avg_word_length']['original']:.2f}, Generated={style_analysis['avg_word_length']['generated']:.2f}\")\n",
    "    print(f\"Vocabulary Diversity: Original={style_analysis['vocabulary_diversity']['original']:.2f}, Generated={style_analysis['vocabulary_diversity']['generated']:.2f}\")\n",
    "    print(f\"Average Sentence Length: Original={style_analysis['avg_sentence_length']['original']:.2f}, Generated={style_analysis['avg_sentence_length']['generated']:.2f}\")\n",
    "    print(f\"Common Words Overlap: {style_analysis['common_words']['overlap_percentage']:.2f}%\")\n",
    "    \n",
    "    # Calculate perplexity for evaluation\n",
    "    perplexity = shakespeare.calculate_perplexity(generated_text_med_temp)\n",
    "    print(f\"\\nPerplexity of generated text: {perplexity:.2f}\")\n",
    "    \n",
    "    print(\"\\nExecution complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
