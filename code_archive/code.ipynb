{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abmir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
    "import unicodedata\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the dataset script to load the dataset\n",
    "dataset = load_dataset(\"tiny_shakespeare.py\", trust_remote_code=True)\n",
    "text = dataset[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the text to a temporary file (tokenizers library needs files)\n",
    "with open(\"shakespeare.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\".join(text))  # Join the list into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# Check out the data\n",
    "print(dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No stemming or removing punctuation because the purpose of this model is to generate text like Shakespeare would. Stemming or removing punctuation would remove the stylistic richness of the data that makes Shakespearean text what it is... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data into Byte-Pair Encoding (BPE) tokens\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.BpeTrainer(vocab_size=5000, special_tokens=[\"<PAD>\",\"<BOS>\", \"<EOS>\", \"<UNK>\"])\n",
    "tokenizer.train([\"shakespeare_clean.txt\"], trainer)\n",
    "tokenizer.decoder = decoders.BPEDecoder()\n",
    "\n",
    "# Save the tokenizer model\n",
    "tokenizer.save(\"shakespeare_BPE_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"shakespeare_BPE_tokenizer.json\")\n",
    "\n",
    "def encode_sequence(text, add_special = True):\n",
    "    if add_special:\n",
    "        text = \"<BOS> \" + text + \" <EOS>\"\n",
    "    return tokenizer.encode(text).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input token IDs: [385, 710, 12, 1854, 115, 2209, 442, 1793, 8, 400, 81, 368, 10, 921, 12, 1936, 8, 368, 10, 385, 710, 12, 325, 162, 140]\n",
      "Sample target token IDs: [710, 12, 1854, 115, 2209, 442, 1793, 8, 400, 81, 368, 10, 921, 12, 1936, 8, 368, 10, 385, 710, 12, 325, 162, 140, 3237]\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(encoded_ids, seq_len=50):\n",
    "    sequences = []\n",
    "    for i in range(len(encoded_ids) - seq_len):\n",
    "        seq = encoded_ids[i:i+seq_len]\n",
    "        target = encoded_ids[i+1:i+seq_len+1]  # next-token prediction\n",
    "        sequences.append((seq, target))\n",
    "    return sequences\n",
    "\n",
    "# Example\n",
    "token_ids = encode_sequence(clean_text, add_special=False)\n",
    "seq_64 = create_sequences(token_ids, seq_len=64)\n",
    "seq_50 = create_sequences(token_ids, seq_len=50)\n",
    "seq_25 = create_sequences(token_ids, seq_len=25)\n",
    "seq_150 = create_sequences(token_ids, seq_len=150)\n",
    "seq_200 = create_sequences(token_ids, seq_len=200)\n",
    "\n",
    "\n",
    "print(\"Sample input token IDs:\", seq_25[0][0])\n",
    "print(\"Sample target token IDs:\", seq_25[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ShakespeareRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, rnn_type='rnn', bidirectional=True):\n",
    "        super(ShakespeareRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn_type = rnn_type.lower()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        # Choose RNN type\n",
    "        if self.rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                batch_first=True,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        elif self.rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                batch_first=True,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        else:  # vanilla RNN\n",
    "            self.rnn = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                batch_first=True,\n",
    "                bidirectional=bidirectional,\n",
    "                nonlinearity='tanh'\n",
    "            )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * self.num_directions, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
    "        output, hidden = self.rnn(x, hidden)  # output: [batch, seq_len, hidden*directions]\n",
    "        output = self.fc(output)  # Predict vocab token at each time step\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 50, 5000])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000  # from tokenizer\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "seq_len = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Try a bidirectional GRU\n",
    "model = ShakespeareRNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    rnn_type='gru',\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "sample_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "output, hidden = model(sample_input)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Should be [batch_size, seq_len, vocab_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "import random\n",
    "\n",
    "# Choose device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Sample hyperparameters\n",
    "vocab_size = 5000\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "seq_len = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Create the model\n",
    "model = ShakespeareRNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    rnn_type='lstm',  # Change to 'gru' or 'rnn' to test\n",
    "    bidirectional=True\n",
    ").to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer â€” swap between Adam and RMSprop\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "# Gradient clipping threshold\n",
    "clip_value = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data, optimizer, criterion, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "\n",
    "        # Skip if final batch is too small\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "\n",
    "        inputs = torch.tensor([x[0] for x in batch], dtype=torch.long).to(device)  # [B, T]\n",
    "        targets = torch.tensor([x[1] for x in batch], dtype=torch.long).to(device)  # [B, T]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(inputs)  # output shape: [B, T, vocab_size]\n",
    "\n",
    "        # Optionally apply teacher forcing (predict one token at a time)\n",
    "        if teacher_forcing_ratio < 1.0:\n",
    "            # Loop through time steps manually\n",
    "            loss = 0\n",
    "            for t in range(seq_len):\n",
    "                use_teacher = random.random() < teacher_forcing_ratio\n",
    "                if use_teacher or t == 0:\n",
    "                    inp = inputs[:, t]\n",
    "                else:\n",
    "                    # Use model's own prediction\n",
    "                    _, top1 = torch.max(output[:, t-1], dim=1)\n",
    "                    inp = top1\n",
    "                pred, _ = model(inp.unsqueeze(1))\n",
    "                loss += criterion(pred.squeeze(1), targets[:, t])\n",
    "            loss = loss / seq_len\n",
    "        else:\n",
    "            # Full-sequence prediction\n",
    "            output = output.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "            loss = criterion(output, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / (len(data) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 1.0914\n",
      "Epoch 2/10 - Loss: 0.1163\n",
      "Epoch 3/10 - Loss: 0.0940\n",
      "Epoch 4/10 - Loss: 0.0850\n",
      "Epoch 5/10 - Loss: 0.0785\n",
      "Epoch 6/10 - Loss: 0.0742\n",
      "Epoch 7/10 - Loss: 0.0713\n",
      "Epoch 8/10 - Loss: 0.0693\n",
      "Epoch 9/10 - Loss: 0.0678\n",
      "Epoch 10/10 - Loss: 0.0670\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, data, epochs=10, optimizer_type='adam', teacher_forcing_ratio=1.0):\n",
    "    # Choose optimizer\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_epoch(model, data, optimizer, criterion, teacher_forcing_ratio)\n",
    "        losses.append(loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "# Train with teacher forcing\n",
    "# tf_losses = train_model(model, seq_50, epochs=10, optimizer_type='adam', teacher_forcing_ratio=1.0)\n",
    "\n",
    "# Train without teacher forcing (set ratio to 0)\n",
    "model = ShakespeareRNN(...).to(device)  # Re-init to reset weights if needed\n",
    "no_tf_losses = train_model(model, seq_50, epochs=10, optimizer_type='adam', teacher_forcing_ratio=0.0)\n",
    "\n",
    "# Plot\n",
    "# plt.plot(tf_losses, label='With Teacher Forcing')\n",
    "# plt.plot(no_tf_losses, label='Without Teacher Forcing')\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Training Loss Comparison\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, start_text=\"<BOS>\", max_length=100, temperature=1.0, top_k=50):\n",
    "    model.eval()\n",
    "\n",
    "    def sample_with_top_k(logits, k):\n",
    "        logits = logits / temperature\n",
    "        probs = torch.nn.functional.softmax(logits, dim=0)\n",
    "        top_probs, top_indices = torch.topk(probs, k)\n",
    "        sampled = torch.multinomial(top_probs, 1)\n",
    "        return top_indices[sampled].item()\n",
    "\n",
    "    input_ids = tokenizer.encode(start_text).ids\n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # shape: [1, seq_len]\n",
    "\n",
    "    generated = input_ids.copy()\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        output, hidden = model(input_tensor, hidden)\n",
    "        logits = output[0, -1, :]  # get last token's logits\n",
    "\n",
    "        # Optional: repetition penalty (commented out by default)\n",
    "        for token_id in set(generated[-10:]):  # penalize repeated tokens in last 10\n",
    "            logits[token_id] *= 0.9\n",
    "\n",
    "        next_token_id = sample_with_top_k(logits, top_k)\n",
    "\n",
    "        if tokenizer.id_to_token(next_token_id) == \"<EOS>\":\n",
    "            break\n",
    "\n",
    "        generated.append(next_token_id)\n",
    "        input_tensor = torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "    return tokenizer.decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m(model, tokenizer, start_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_text' is not defined"
     ]
    }
   ],
   "source": [
    "generated = generate_text(model, tokenizer, start_text=\"To be\", max_length=100, temperature= 1, top_k=50)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
