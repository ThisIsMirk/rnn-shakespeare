{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc6AZEm670uB",
        "outputId": "bc97d089-fff9-4d65-beee-dfd50705b29b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\abmir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
        "import unicodedata\n",
        "import os\n",
        "import re\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.normalizers import NFKC\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from tokenizers.decoders import BPEDecoder\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HC4vTLfF70uN"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    lines = text.split(\"\\n\")\n",
        "    cleaned_lines = [unicodedata.normalize(\"NFKC\", line).strip() for line in lines if line.strip()]\n",
        "    return \"\\n\".join(cleaned_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bDF-rjYOzMiX"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"tiny_shakespeare.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1XVVkDjOHeow"
      },
      "outputs": [],
      "source": [
        "drive_path = \"/content/drive/MyDrive/Assignment 2 - MGSC695/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N44kaIjDDzSW"
      },
      "outputs": [],
      "source": [
        "train_data = dataset[\"train\"][\"text\"]\n",
        "valid_data = dataset[\"validation\"][\"text\"]\n",
        "test_data = dataset[\"test\"][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D16MSdeiGPZr",
        "outputId": "502c4828-1c2f-4101-e38a-016141a40a37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "All:\n",
            "Speak, speak.\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "All:\n",
            "Resolved. resolved.\n",
            "First Citizen:\n",
            "First, you kno\n"
          ]
        }
      ],
      "source": [
        "# store the text in variables\n",
        "train_text = \" \".join(train_data)\n",
        "valid_text = \" \".join(valid_data)\n",
        "test_text = \" \".join(test_data)\n",
        "\n",
        "# preprocess the text\n",
        "train_text = preprocess(train_text)\n",
        "valid_text = preprocess(valid_text)\n",
        "test_text = preprocess(test_text)\n",
        "print(train_text[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7WmpwNdS70uP"
      },
      "outputs": [],
      "source": [
        "# Write the training text to a temporary file (tokenizers library needs files)\n",
        "with open(\"shakespeare_clean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXPh5_P7HLoH",
        "outputId": "f075556e-6dcd-4849-c93f-2349fa0f6f45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rance ta'en\n",
            "As shall with either part's agreement stand?\n",
            "BAPTISTA:\n",
            "Not in my house, Lucentio; for, you know,\n",
            "Pitchers have ears, and I have many servants:\n",
            "Besides, old Gremio is hearkening still;\n",
            "And \n"
          ]
        }
      ],
      "source": [
        "# check out what the data looks like\n",
        "print(test_text[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-StfesZF70uT"
      },
      "source": [
        "### No stemming or removing punctuation because the purpose of this model is to generate text like Shakespeare would. Stemming or removing punctuation would remove the stylistic richness of the data that makes Shakespearean text what it is..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SfBOAgZSMcM3"
      },
      "outputs": [],
      "source": [
        "train_lines = train_text.split(\"\\n\")\n",
        "valid_lines = valid_text.split(\"\\n\")\n",
        "test_lines = test_text.split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9I9B0RUN70uY"
      },
      "outputs": [],
      "source": [
        "# Define the tokenizer and trainer\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "tokenizer.normalizer = NFKC()\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0hgqrxid70uZ"
      },
      "outputs": [],
      "source": [
        "# Define trainer\n",
        "trainer = BpeTrainer(\n",
        "    vocab_size=1000,  # You can change this based on your dataset size\n",
        "    special_tokens=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        ")\n",
        "\n",
        "# Train on training data only\n",
        "tokenizer.train_from_iterator(train_lines, trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ebr6YDSbM_fS"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"<bos> $A <eos>\",\n",
        "    special_tokens=[\n",
        "        (\"<bos>\", tokenizer.token_to_id(\"<bos>\")),\n",
        "        (\"<eos>\", tokenizer.token_to_id(\"<eos>\"))\n",
        "    ]\n",
        ")\n",
        "tokenizer.decoder = BPEDecoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KCAC5nCzM_Q9"
      },
      "outputs": [],
      "source": [
        "# Tokenize each dataset\n",
        "train_encoded = tokenizer.encode(train_text)\n",
        "val_encoded = tokenizer.encode(valid_text)\n",
        "test_encoded = tokenizer.encode(test_text)\n",
        "\n",
        "# Get token ID sequences\n",
        "train_ids = train_encoded.ids\n",
        "val_ids = val_encoded.ids\n",
        "test_ids = test_encoded.ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jgV8-avLNaLe"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(\"bpe_tokenizer.json\")\n",
        "\n",
        "# if we need it later:\n",
        "# tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-BzKHVFpNq7q"
      },
      "outputs": [],
      "source": [
        "# time for creating input and target sequences\n",
        "seq_len = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iT7UPDilO96u"
      },
      "outputs": [],
      "source": [
        "def create_sequences(data_ids, seq_len):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(0, len(data_ids) - seq_len):\n",
        "        inputs.append(data_ids[i:i+seq_len])\n",
        "        targets.append(data_ids[i+1:i+seq_len+1])\n",
        "    return inputs, targets\n",
        "\n",
        "X_train, y_train = create_sequences(train_ids, seq_len)\n",
        "X_val, y_val = create_sequences(val_ids, seq_len)\n",
        "X_test, y_test = create_sequences(test_ids, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "f3RExefvPS_C"
      },
      "outputs": [],
      "source": [
        "# Convert to tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "# Build a dataset and dataloader\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wHnr1NYDUGKP"
      },
      "outputs": [],
      "source": [
        "X_val = torch.tensor(X_val, dtype=torch.long)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LNRzDeiwQb4y"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class VanillaRNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, bidirectional=True):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=tokenizer.token_to_id(\"<pad>\"))\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=embed_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        self.fc = nn.Linear(hidden_size * self.num_directions, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        output, hidden = self.rnn(x, hidden)\n",
        "        logits = self.fc(output)\n",
        "        return logits, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vIiCOn1c8RNN"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.get_vocab_size()\n",
        "model = VanillaRNNModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_size=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    bidirectional=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "eiDULYu9TV5J"
      },
      "outputs": [],
      "source": [
        "# use GPU only\n",
        "assert torch.cuda.is_available(), \"CUDA is not available. Please run on a machine with a GPU.\"\n",
        "device = torch.device(\"cuda\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "nwhSgteITdng"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 ‚Äî Train Loss: 0.1308 ‚Äî Val Loss: 0.0933\n",
            "Epoch 2/5 ‚Äî Train Loss: 0.0876 ‚Äî Val Loss: 0.0914\n",
            "Epoch 3/5 ‚Äî Train Loss: 0.0841 ‚Äî Val Loss: 0.0917\n",
            "Epoch 4/5 ‚Äî Train Loss: 0.0821 ‚Äî Val Loss: 0.0912\n",
            "Epoch 5/5 ‚Äî Train Loss: 0.0808 ‚Äî Val Loss: 0.0915\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 5\n",
        "clip_value = 1.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(X_batch)\n",
        "\n",
        "        loss = criterion(outputs.view(-1, vocab_size), y_batch.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # üîç VALIDATION\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs, _ = model(X_batch)\n",
        "\n",
        "            val_loss = criterion(outputs.view(-1, vocab_size), y_batch.view(-1))\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} ‚Äî Train Loss: {avg_train_loss:.4f} ‚Äî Val Loss: {avg_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model to file\n",
        "torch.save(model.state_dict(),\"vanilla_rnn_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, max_length=100, temperature=1.0):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Encode the prompt into token IDs\n",
        "    encoded = tokenizer.encode(prompt)\n",
        "    input_ids = torch.tensor(encoded.ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # Hidden state (can be None initially)\n",
        "    hidden = None\n",
        "    generated_ids = input_ids.tolist()[0]  # seed with initial prompt tokens\n",
        "\n",
        "    # Generate tokens one-by-one\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model(input_ids, hidden)  # output: [1, seq_len, vocab_size]\n",
        "            logits = output[:, -1, :] / temperature    # take last token's logits\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            generated_ids.append(next_token_id)\n",
        "\n",
        "            input_ids = torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
        "\n",
        "    # Decode generated token IDs back into text\n",
        "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìú Generated Text:\n",
            "Tobe,ornottowereupzfalullboypleckveveryrighttalGELhiconf,ComekindwereCORIOLANLOortWellqueenarsspsorItsweRICHVIhimtleerfirstIZicefeIIachherefoothingELIZABETHTisakelorddedowerparThisthineyoujeHENRYVGounetingtrongheaWARWICcomeunwerdescomesfromconfbedYetShallOMThisWillwhinatherWICcondbeheardhandssoForENsandfaroughtRICHvenhitherprisentHeffriendsoulGLUCYorkguquiurWARWICtelluntoUSithueper?ousingosetEOusKINGHAMfal\n"
          ]
        }
      ],
      "source": [
        "prompt = \"To be, or not to\"\n",
        "output = generate_text(model, tokenizer, prompt, max_length=120, temperature=1.5)\n",
        "\n",
        "print(\"üìú Generated Text:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=tokenizer.token_to_id(\"<pad>\"))\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        self.fc = nn.Linear(hidden_size * self.num_directions, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is not None:\n",
        "            output, hidden = self.lstm(x, hidden)\n",
        "        else:\n",
        "            output, hidden = self.lstm(x)\n",
        "        logits = self.fc(output)\n",
        "        return logits, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMLanguageModel(\n",
              "  (embedding): Embedding(1000, 512, padding_idx=1)\n",
              "  (lstm): LSTM(512, 1024, num_layers=3, batch_first=True)\n",
              "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "model_LSTM = LSTMLanguageModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_size=512,  # increased from 256\n",
        "    hidden_size=1024,  # increased from 512\n",
        "    num_layers=3,  # increased from 2\n",
        "    bidirectional=False\n",
        ").to(device)\n",
        "\n",
        "# Initialize new optimizer specifically for LSTM\n",
        "optimizer_lstm = torch.optim.Adam(model_LSTM.parameters(), lr=0.001)\n",
        "# Add learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_lstm, mode='min', factor=0.5, patience=1)\n",
        "\n",
        "# Initialize weights properly\n",
        "def init_weights(m):\n",
        "    if type(m) in [nn.Linear, nn.Embedding]:\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "model_LSTM.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model_LSTM\u001b[38;5;241m.\u001b[39mparameters(), clip_value)\n\u001b[0;32m     20\u001b[0m     optimizer_lstm\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 22\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     26\u001b[0m model_LSTM\u001b[38;5;241m.\u001b[39meval()\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 10  # increased from 5\n",
        "clip_value = 0.25  # reduced from 1.0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_LSTM.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer_lstm.zero_grad()  # use lstm optimizer\n",
        "        outputs, _ = model_LSTM(X_batch)\n",
        "\n",
        "        loss = criterion(outputs.view(-1, vocab_size), y_batch.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model_LSTM.parameters(), clip_value)\n",
        "        optimizer_lstm.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    model_LSTM.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs, _ = model_LSTM(X_batch)\n",
        "            val_loss = criterion(outputs.view(-1, vocab_size), y_batch.view(-1))\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    \n",
        "    # Update learning rate based on validation loss\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model_LSTM.state_dict(), 'best_lstm_model.pth')\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} ‚Äî Train Loss: {avg_train_loss:.4f} ‚Äî Val Loss: {avg_val_loss:.4f} ‚Äî LR: {optimizer_lstm.param_groups[0]['lr']:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìú Generated Text:\n",
            "Tobe,ornottoCEINghanYourengENIUSrvesgrcallsuchtisIOLANulGARwonightanceqENIfbronolifeearthsweedqueendsARDTocrownProchildzVkEDWcannotwilllordsNurserenfriendsEDWARDgCORIOLANUSukeilCAPULETableisonearsblefiWARButawayLUCNorfacedracleEDWULhereOMlThousterRimarowKingcondzenWarwickhytelovethatTERDUKEinesssayXLUCshOLouldYorELslaUCESTERvedthrCongIVfollowprayISABELLAshallxHtislackgueDUKEornagainMARGARheapleORDinfearHowfear\n"
          ]
        }
      ],
      "source": [
        "prompt = \"To be, or not to\"\n",
        "output = generate_text(model_LSTM, tokenizer, prompt, max_length=120, temperature=1.1)\n",
        "\n",
        "print(\"üìú Generated Text:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model\n",
        "torch.save(model_LSTM.state_dict(),\"lstm_model.pth\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
