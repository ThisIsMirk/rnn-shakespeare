{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "\n",
        "\n",
        "# Check if CUDA is available, otherwise use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "# 1. Load and Preprocess Data\n",
        "def load_tiny_shakespeare():\n",
        "    \"\"\"\n",
        "    Simulate loading data from tiny_shakespeare.py\n",
        "    Normally you would import the module, but we'll simulate the dataset structure\n",
        "    \"\"\"\n",
        "    # This would normally be imported from tiny_shakespeare.py\n",
        "    try:\n",
        "        with open('shakespeare_clean.txt', 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "    except FileNotFoundError:\n",
        "        # Sample text for demonstration\n",
        "        text = \"\"\"\n",
        "        To be, or not to be, that is the question:\n",
        "        Whether 'tis nobler in the mind to suffer\n",
        "        The slings and arrows of outrageous fortune,\n",
        "        Or to take arms against a sea of troubles\n",
        "        And by opposing end them.\n",
        "        \"\"\"\n",
        "        print(\"Using sample text since tiny_shakespeare.txt was not found\")\n",
        "\n",
        "    # Split into train, val, test (90%, 5%, 5%)\n",
        "    train_end = int(len(text) * 0.9)\n",
        "    val_end = int(len(text) * 0.95)\n",
        "\n",
        "    return {\n",
        "        'train': text[:train_end],\n",
        "        'val': text[train_end:val_end],\n",
        "        'test': text[val_end:]\n",
        "    }\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def preprocess(text):\n",
        "    # Normalize to standard Unicode form (avoids weird encodings)\n",
        "    lines = text.split(\"\\n\")\n",
        "    cleaned_lines = [\n",
        "        unicodedata.normalize(\"NFKC\", line).strip()\n",
        "        for line in lines\n",
        "        if line.strip()  # Remove empty lines\n",
        "    ]\n",
        "    return \"\\n\".join(cleaned_lines)\n",
        "def create_bpe_tokenizer(text, vocab_size=2000):\n",
        "    \"\"\"Create a BPE tokenizer with the specified vocabulary size\"\"\"\n",
        "    # Initialize a BPE tokenizer\n",
        "    tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "    # Configure pre-tokenization (how to split text into initial tokens)\n",
        "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "    # Train the tokenizer\n",
        "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "    tokenizer.train_from_iterator([text], trainer=trainer)\n",
        "\n",
        "\n",
        "    tokenizer.save(\"bpe_tokenizer.json\")\n",
        "\n",
        "    return tokenizer\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text, tokenizer, seq_length=100):\n",
        "        self.seq_length = seq_length\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Tokenize the text\n",
        "        encoding = tokenizer.encode(text)\n",
        "        self.tokens = encoding.ids\n",
        "        self.vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "        print(f\"Loaded {len(self.tokens)} tokens with vocabulary size {self.vocab_size}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, len(self.tokens) - self.seq_length - 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get a sequence and the next token as target\"\"\"\n",
        "        # Input sequence\n",
        "        input_seq = self.tokens[idx:idx + self.seq_length]\n",
        "        # Target sequence (shifted by 1)\n",
        "        target_seq = self.tokens[idx + 1:idx + self.seq_length + 1]\n",
        "\n",
        "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, bidirectional=True):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_dim * self.num_directions, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # x shape: (batch_size, seq_length)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Initialize hidden and cell states if not provided\n",
        "        if hidden is None:\n",
        "            h0 = torch.zeros(self.num_directions, batch_size, self.hidden_dim).to(device)\n",
        "            c0 = torch.zeros(self.num_directions, batch_size, self.hidden_dim).to(device)\n",
        "            hidden = (h0, c0)\n",
        "\n",
        "        # Apply embedding\n",
        "        embed = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        output, hidden = self.lstm(embed, hidden)\n",
        "\n",
        "        # Pass through linear layer\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h0 = torch.zeros(self.num_directions, batch_size, self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_directions, batch_size, self.hidden_dim).to(device)\n",
        "        return (h0, c0)\n",
        "def train_model(model, train_dataset, val_dataset, batch_size=32, num_epochs=5, learning_rate=0.001, clip_value=1.0, teacher_forcing_ratio=0.5):\n",
        "    \"\"\"Train the RNN model with validation and teacher forcing\"\"\"\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Use Adam optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_perplexities = []\n",
        "    val_perplexities = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            batch_size = inputs.size(0)\n",
        "            seq_length = inputs.size(1)\n",
        "\n",
        "            # Initialize hidden state\n",
        "            hidden = model.init_hidden(batch_size)\n",
        "\n",
        "            # Reset gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Initialize output tensor\n",
        "            outputs = torch.zeros(batch_size, seq_length, model.vocab_size).to(device)\n",
        "\n",
        "            # Teacher forcing with probability teacher_forcing_ratio\n",
        "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            if use_teacher_forcing:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = inputs\n",
        "                output, hidden = model(decoder_input, hidden)\n",
        "                outputs = output\n",
        "            else:\n",
        "                # Without teacher forcing: use own predictions as the next input\n",
        "                decoder_input = inputs[:, 0].unsqueeze(1)\n",
        "                for t in range(seq_length):\n",
        "                    output, hidden = model(decoder_input, hidden)\n",
        "                    outputs[:, t:t+1] = output[:, -1:, :]\n",
        "                    decoder_input = output[:, -1:, :].argmax(2)\n",
        "\n",
        "            # Reshape outputs and targets for loss calculation\n",
        "            outputs = outputs.reshape(-1, outputs.shape[-1])\n",
        "            targets = targets.reshape(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Print progress (less frequently to reduce output)\n",
        "            if (batch_idx + 1) % 50 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Calculate average training loss and perplexity\n",
        "        avg_train_loss = epoch_loss / len(train_loader)\n",
        "        train_perplexity = np.exp(avg_train_loss)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_perplexities.append(train_perplexity)\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                # Initialize hidden state\n",
        "                hidden = model.init_hidden(inputs.size(0))\n",
        "\n",
        "                outputs, _ = model(inputs, hidden)\n",
        "                outputs = outputs.reshape(-1, outputs.shape[-1])\n",
        "                targets = targets.reshape(-1)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Calculate average validation loss and perplexity\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_perplexity = np.exp(avg_val_loss)\n",
        "\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_perplexities.append(val_perplexity)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Perplexity: {train_perplexity:.4f}, '\n",
        "              f'Val Loss: {avg_val_loss:.4f}, Val Perplexity: {val_perplexity:.4f}')\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Val Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_perplexities, label='Train Perplexity')\n",
        "    plt.plot(val_perplexities, label='Val Perplexity')\n",
        "    plt.legend()\n",
        "    plt.title('Perplexity')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_plot.png')\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_perplexities': train_perplexities,\n",
        "        'val_perplexities': val_perplexities\n",
        "    }\n",
        "def generate_text(model, tokenizer, seed_text='To be, or not to be', gen_length=100, temperature=0.8):\n",
        "    \"\"\"Generate text using the trained model\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize seed text\n",
        "    tokens = tokenizer.encode(seed_text).ids\n",
        "    input_seq = torch.tensor([tokens], dtype=torch.long).to(device)\n",
        "\n",
        "    # Initialize hidden state\n",
        "    hidden = model.init_hidden(1)\n",
        "\n",
        "    # Generate text\n",
        "    generated_tokens = tokens.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(gen_length):\n",
        "            # Get prediction for next token\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "\n",
        "            # Apply temperature\n",
        "            logits = output[:, -1, :] / temperature\n",
        "            probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample next token\n",
        "            next_token = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "            # Add to generated tokens\n",
        "            generated_tokens.append(next_token)\n",
        "\n",
        "            # Update input for next iteration (use the last predicted token)\n",
        "            input_seq = torch.tensor([[next_token]], dtype=torch.long).to(device)\n",
        "\n",
        "    # Decode generated tokens\n",
        "    generated_text = tokenizer.decode(generated_tokens)\n",
        "    return generated_text\n",
        "def generate_text_beam_search(model, tokenizer, seed_text='To be, or not to be', gen_length=100, beam_width=5, temperature=0.8):\n",
        "    \"\"\"Generate text using beam search decoding\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize seed text\n",
        "    tokens = tokenizer.encode(seed_text).ids\n",
        "    input_seq = torch.tensor([tokens], dtype=torch.long).to(device)\n",
        "\n",
        "    # Initialize beam search\n",
        "    beams = [(input_seq, model.init_hidden(1), 0.0)]  # (sequence, hidden_state, log_prob)\n",
        "    finished_beams = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(gen_length):\n",
        "            candidates = []\n",
        "\n",
        "            # Expand each beam\n",
        "            for seq, hidden, log_prob in beams:\n",
        "                # Get prediction for next token\n",
        "                output, new_hidden = model(seq[:, -1:], hidden)\n",
        "                logits = output[:, -1, :] / temperature\n",
        "                probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "                # Get top-k candidates\n",
        "                top_probs, top_tokens = probs.topk(beam_width)\n",
        "\n",
        "                for prob, token in zip(top_probs[0], top_tokens[0]):\n",
        "                    new_seq = torch.cat([seq, token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "                    new_log_prob = log_prob + prob.item()\n",
        "                    candidates.append((new_seq, new_hidden, new_log_prob))\n",
        "\n",
        "            # Select top beams\n",
        "            candidates.sort(key=lambda x: x[2], reverse=True)\n",
        "            beams = candidates[:beam_width]\n",
        "\n",
        "        # Select best sequence\n",
        "        best_seq = max(beams, key=lambda x: x[2])[0]\n",
        "        generated_tokens = best_seq[0].tolist()\n",
        "\n",
        "    # Decode generated tokens\n",
        "    generated_text = tokenizer.decode(generated_tokens)\n",
        "    return generated_text\n",
        "def evaluate_perplexity(model, dataset, batch_size=32):\n",
        "    \"\"\"Evaluate model perplexity on a dataset\"\"\"\n",
        "    model.eval()\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            batch_size = inputs.size(0)\n",
        "\n",
        "            # Initialize hidden state\n",
        "            hidden = model.init_hidden(batch_size)\n",
        "\n",
        "            outputs, _ = model(inputs, hidden)\n",
        "            outputs = outputs.reshape(-1, outputs.shape[-1])\n",
        "            targets = targets.reshape(-1)\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item() * targets.size(0)\n",
        "            total_tokens += targets.size(0)\n",
        "\n",
        "    # Calculate perplexity\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = np.exp(avg_loss)\n",
        "\n",
        "    return perplexity\n",
        "# Load data\n",
        "print(\"Loading Shakespeare data...\")\n",
        "data = load_tiny_shakespeare()\n",
        "\n",
        "# Create BPE tokenizer (smaller vocab size for faster training)\n",
        "print(\"Creating BPE tokenizer...\")\n",
        "tokenizer = create_bpe_tokenizer(data['train'], vocab_size=1000)\n",
        "\n",
        "# Create datasets\n",
        "print(\"Preparing datasets...\")\n",
        "seq_length = 100  # Using fixed sequence length for simplicity\n",
        "train_dataset = TextDataset(data['train'], tokenizer, seq_length)\n",
        "val_dataset = TextDataset(data['val'], tokenizer, seq_length)\n",
        "test_dataset = TextDataset(data['test'], tokenizer, seq_length)\n",
        "\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Create models (with smaller sizes for lightweight training)\n",
        "# rnn_model = VanillaRNN(vocab_size, embedding_dim=64, hidden_dim=128, bidirectional=True)\n",
        "lstm_model = SimpleLSTM(vocab_size, embedding_dim=64, hidden_dim=128, bidirectional=True)\n",
        "\n",
        "# Train models (reduced epochs and batch size for faster training)\n",
        "# print(\"\\nTraining Vanilla RNN model...\")\n",
        "# rnn_results = train_model(rnn_model, train_dataset, val_dataset,\n",
        "#                          batch_size=16, num_epochs=3, learning_rate=0.001)\n",
        "\n",
        "print(\"\\nTraining LSTM model...\")\n",
        "lstm_results = train_model(lstm_model, train_dataset, val_dataset,\n",
        "                        batch_size=64,  # Increased from 16 to 64\n",
        "                        num_epochs=1,\n",
        "                        learning_rate=0.01,\n",
        "                        teacher_forcing_ratio=0.5)\n",
        "\n",
        "# Evaluate models on test set\n",
        "print(\"\\nEvaluating models...\")\n",
        "# rnn_perplexity = evaluate_perplexity(rnn_model, test_dataset)\n",
        "lstm_perplexity = evaluate_perplexity(lstm_model, test_dataset)\n",
        "\n",
        "# print(f\"Vanilla RNN Test Perplexity: {rnn_perplexity:.4f}\")\n",
        "print(f\"LSTM Test Perplexity: {lstm_perplexity:.4f}\")\n",
        "\n",
        "# Compare models\n",
        "model_results = {\n",
        "# 'Vanilla RNN': rnn_results,\n",
        "'LSTM': lstm_results\n",
        "}\n",
        "# compare_models(model_results)\n",
        "\n",
        "# Generate text samples\n",
        "# print(\"\\nGenerating text with RNN model:\")\n",
        "# rnn_text = generate_text(rnn_model, tokenizer, seed_text=\"To be, or not to be\", gen_length=100)\n",
        "# print(rnn_text)\n",
        "\n",
        "print(\"\\nGenerating text with LSTM model:\")\n",
        "lstm_text = generate_text(lstm_model, tokenizer, seed_text=\"To be, or not to be\", gen_length=100)\n",
        "print(lstm_text)\n",
        "\n",
        "# Generate text using beam search\n",
        "print(\"\\nGenerating text with LSTM model using beam search:\")\n",
        "lstm_text_beam = generate_text_beam_search(lstm_model, tokenizer,\n",
        "                                          seed_text=\"To be, or not to be\",\n",
        "                                          gen_length=100,\n",
        "                                          beam_width=5)\n",
        "print(lstm_text_beam)\n",
        "# save the LSTM model\n",
        "torch.save(lstm_model.state_dict(), 'lstm_model_codev4.0.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyItclaFBXam",
        "outputId": "2635b46a-d962-4d02-e3da-c51bd5ce0783"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading Shakespeare data...\n",
            "Creating BPE tokenizer...\n",
            "Preparing datasets...\n",
            "Loaded 307825 tokens with vocabulary size 1000\n",
            "Loaded 17797 tokens with vocabulary size 1000\n",
            "Loaded 18338 tokens with vocabulary size 1000\n",
            "Vocabulary size: 1000\n",
            "\n",
            "Training LSTM model...\n",
            "Epoch 1/1, Batch 50/4809, Loss: 6.0977\n",
            "Epoch 1/1, Batch 100/4809, Loss: 1.1025\n",
            "Epoch 1/1, Batch 150/4809, Loss: 0.5645\n",
            "Epoch 1/1, Batch 200/4809, Loss: 6.0588\n",
            "Epoch 1/1, Batch 250/4809, Loss: 0.1867\n",
            "Epoch 1/1, Batch 300/4809, Loss: 6.1620\n",
            "Epoch 1/1, Batch 350/4809, Loss: 5.9735\n",
            "Epoch 1/1, Batch 400/4809, Loss: 5.9815\n",
            "Epoch 1/1, Batch 450/4809, Loss: 5.9569\n",
            "Epoch 1/1, Batch 500/4809, Loss: 0.0985\n",
            "Epoch 1/1, Batch 550/4809, Loss: 5.9903\n",
            "Epoch 1/1, Batch 600/4809, Loss: 0.0908\n",
            "Epoch 1/1, Batch 650/4809, Loss: 0.0973\n",
            "Epoch 1/1, Batch 700/4809, Loss: 6.0516\n",
            "Epoch 1/1, Batch 750/4809, Loss: 0.0978\n",
            "Epoch 1/1, Batch 800/4809, Loss: 0.0918\n",
            "Epoch 1/1, Batch 850/4809, Loss: 5.9336\n",
            "Epoch 1/1, Batch 900/4809, Loss: 0.0976\n",
            "Epoch 1/1, Batch 950/4809, Loss: 5.9252\n",
            "Epoch 1/1, Batch 1000/4809, Loss: 0.0988\n",
            "Epoch 1/1, Batch 1050/4809, Loss: 0.0837\n",
            "Epoch 1/1, Batch 1100/4809, Loss: 0.0821\n",
            "Epoch 1/1, Batch 1150/4809, Loss: 5.9199\n",
            "Epoch 1/1, Batch 1200/4809, Loss: 0.0809\n",
            "Epoch 1/1, Batch 1250/4809, Loss: 0.0822\n",
            "Epoch 1/1, Batch 1300/4809, Loss: 5.9047\n",
            "Epoch 1/1, Batch 1350/4809, Loss: 5.9367\n",
            "Epoch 1/1, Batch 1400/4809, Loss: 0.0740\n",
            "Epoch 1/1, Batch 1450/4809, Loss: 5.8944\n",
            "Epoch 1/1, Batch 1500/4809, Loss: 0.1009\n",
            "Epoch 1/1, Batch 1550/4809, Loss: 0.0706\n",
            "Epoch 1/1, Batch 1600/4809, Loss: 0.0687\n",
            "Epoch 1/1, Batch 1650/4809, Loss: 5.9494\n",
            "Epoch 1/1, Batch 1700/4809, Loss: 0.0636\n",
            "Epoch 1/1, Batch 1750/4809, Loss: 0.0678\n",
            "Epoch 1/1, Batch 1800/4809, Loss: 0.0644\n",
            "Epoch 1/1, Batch 1850/4809, Loss: 5.9574\n",
            "Epoch 1/1, Batch 1900/4809, Loss: 5.9479\n",
            "Epoch 1/1, Batch 1950/4809, Loss: 5.9781\n",
            "Epoch 1/1, Batch 2000/4809, Loss: 5.9402\n",
            "Epoch 1/1, Batch 2050/4809, Loss: 0.0681\n",
            "Epoch 1/1, Batch 2100/4809, Loss: 5.9385\n",
            "Epoch 1/1, Batch 2150/4809, Loss: 0.0666\n",
            "Epoch 1/1, Batch 2200/4809, Loss: 5.9371\n",
            "Epoch 1/1, Batch 2250/4809, Loss: 5.9200\n",
            "Epoch 1/1, Batch 2300/4809, Loss: 0.0690\n",
            "Epoch 1/1, Batch 2350/4809, Loss: 6.0217\n",
            "Epoch 1/1, Batch 2400/4809, Loss: 5.9068\n",
            "Epoch 1/1, Batch 2450/4809, Loss: 5.9399\n",
            "Epoch 1/1, Batch 2500/4809, Loss: 5.9021\n",
            "Epoch 1/1, Batch 2550/4809, Loss: 0.0622\n",
            "Epoch 1/1, Batch 2600/4809, Loss: 5.9402\n",
            "Epoch 1/1, Batch 2650/4809, Loss: 0.0777\n",
            "Epoch 1/1, Batch 2700/4809, Loss: 5.9264\n",
            "Epoch 1/1, Batch 2750/4809, Loss: 5.9387\n",
            "Epoch 1/1, Batch 2800/4809, Loss: 0.0764\n",
            "Epoch 1/1, Batch 2850/4809, Loss: 5.9263\n",
            "Epoch 1/1, Batch 2900/4809, Loss: 5.9750\n",
            "Epoch 1/1, Batch 2950/4809, Loss: 0.0572\n",
            "Epoch 1/1, Batch 3000/4809, Loss: 0.0626\n",
            "Epoch 1/1, Batch 3050/4809, Loss: 5.9928\n",
            "Epoch 1/1, Batch 3100/4809, Loss: 0.0729\n",
            "Epoch 1/1, Batch 3150/4809, Loss: 0.1048\n",
            "Epoch 1/1, Batch 3200/4809, Loss: 0.1034\n",
            "Epoch 1/1, Batch 3250/4809, Loss: 0.0961\n",
            "Epoch 1/1, Batch 3300/4809, Loss: 6.0320\n",
            "Epoch 1/1, Batch 3350/4809, Loss: 5.9619\n",
            "Epoch 1/1, Batch 3400/4809, Loss: 0.0675\n",
            "Epoch 1/1, Batch 3450/4809, Loss: 5.9128\n",
            "Epoch 1/1, Batch 3500/4809, Loss: 5.9683\n",
            "Epoch 1/1, Batch 3550/4809, Loss: 0.0586\n",
            "Epoch 1/1, Batch 3600/4809, Loss: 5.9363\n",
            "Epoch 1/1, Batch 3650/4809, Loss: 0.0561\n",
            "Epoch 1/1, Batch 3700/4809, Loss: 5.9413\n",
            "Epoch 1/1, Batch 3750/4809, Loss: 5.9430\n",
            "Epoch 1/1, Batch 3800/4809, Loss: 0.0794\n",
            "Epoch 1/1, Batch 3850/4809, Loss: 0.0736\n",
            "Epoch 1/1, Batch 3900/4809, Loss: 5.9686\n",
            "Epoch 1/1, Batch 3950/4809, Loss: 5.9151\n",
            "Epoch 1/1, Batch 4000/4809, Loss: 0.0715\n",
            "Epoch 1/1, Batch 4050/4809, Loss: 0.0735\n",
            "Epoch 1/1, Batch 4100/4809, Loss: 5.9997\n",
            "Epoch 1/1, Batch 4150/4809, Loss: 5.9731\n",
            "Epoch 1/1, Batch 4200/4809, Loss: 5.9425\n",
            "Epoch 1/1, Batch 4250/4809, Loss: 0.0760\n",
            "Epoch 1/1, Batch 4300/4809, Loss: 5.9393\n",
            "Epoch 1/1, Batch 4350/4809, Loss: 5.9611\n",
            "Epoch 1/1, Batch 4400/4809, Loss: 5.9278\n",
            "Epoch 1/1, Batch 4450/4809, Loss: 5.9276\n",
            "Epoch 1/1, Batch 4500/4809, Loss: 0.0780\n",
            "Epoch 1/1, Batch 4550/4809, Loss: 5.9581\n",
            "Epoch 1/1, Batch 4600/4809, Loss: 5.9563\n",
            "Epoch 1/1, Batch 4650/4809, Loss: 0.1086\n",
            "Epoch 1/1, Batch 4700/4809, Loss: 0.0689\n",
            "Epoch 1/1, Batch 4750/4809, Loss: 5.9232\n",
            "Epoch 1/1, Batch 4800/4809, Loss: 0.0693\n",
            "Epoch 1/1, Train Loss: 3.0699, Train Perplexity: 21.5394, Val Loss: 0.1187, Val Perplexity: 1.1260\n",
            "\n",
            "Evaluating models...\n",
            "LSTM Test Perplexity: 1.1600\n",
            "\n",
            "Generating text with LSTM model:\n",
            "To be , or not to be ll To p what bl what bl own : -- What t f r , s S h you con t ish h ish men Or men m ore , I ' now y , , For your . . . r ' ge but of w in his brother G ha me ro pe : , some ti lt ti lt ti lt ti : It well be en as your as your arm a s ing , the of hea n er of I our k - m ir m ir m b led : I est were\n",
            "\n",
            "Generating text with LSTM model using beam search:\n",
            "To be , or not to be to be to be to ' d , . I , : I , , , : I , , , : I , , , : I , , , : I , , , : I , , , : I , , , : I , , , : I , , , : I , , , : I , , , : I , , , : I , , , : I , , , : I , , , : I , , , : I , , , : I , ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, bidirectional=True):\n",
        "        super(VanillaRNN, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # RNN layer\n",
        "        self.rnn = nn.RNN(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_dim * self.num_directions, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # x shape: (batch_size, seq_length)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Initialize hidden state if not provided\n",
        "        if hidden is None:\n",
        "            hidden = torch.zeros(self.num_directions, batch_size, self.hidden_dim).to(device)\n",
        "\n",
        "        # Apply embedding\n",
        "        embed = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # Pass through RNN\n",
        "        output, hidden = self.rnn(embed, hidden)\n",
        "\n",
        "        # Pass through linear layer\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_directions, batch_size, self.hidden_dim).to(device)"
      ],
      "metadata": {
        "id": "vvd9kv56ViQK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model = VanillaRNN(vocab_size, embedding_dim=64, hidden_dim=128, bidirectional=True)\n",
        "\n",
        "\n",
        "# Train models (reduced epochs and batch size for faster training)\n",
        "print(\"\\nTraining Vanilla RNN model...\")\n",
        "rnn_results = train_model(rnn_model, train_dataset, val_dataset,\n",
        "                          batch_size=64, num_epochs=1, learning_rate=0.01)\n",
        "\n",
        "\n",
        "# Evaluate models on test set\n",
        "print(\"\\nEvaluating models...\")\n",
        "rnn_perplexity = evaluate_perplexity(rnn_model, test_dataset)\n",
        "lstm_perplexity = evaluate_perplexity(lstm_model, test_dataset)\n",
        "\n",
        "print(f\"Vanilla RNN Test Perplexity: {rnn_perplexity:.4f}\")\n",
        "print(f\"LSTM Test Perplexity: {lstm_perplexity:.4f}\")\n",
        "\n",
        "# Compare models\n",
        "model_results = {\n",
        "'Vanilla RNN': rnn_results,\n",
        "'LSTM': lstm_results\n",
        "}\n",
        "# compare_models(model_results)\n",
        "\n",
        "# Generate text samples\n",
        "print(\"\\nGenerating text with RNN model:\")\n",
        "rnn_text = generate_text(rnn_model, tokenizer, seed_text=\"To be, or not to be\", gen_length=100)\n",
        "print(rnn_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUeJaDYGVN1j",
        "outputId": "7eba7c44-132e-4ea9-9a20-ef8f37d6aa0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Vanilla RNN model...\n",
            "Epoch 1/1, Batch 50/4809, Loss: 2.8348\n",
            "Epoch 1/1, Batch 100/4809, Loss: 6.5197\n",
            "Epoch 1/1, Batch 150/4809, Loss: 6.6387\n",
            "Epoch 1/1, Batch 200/4809, Loss: 0.6402\n",
            "Epoch 1/1, Batch 250/4809, Loss: 0.3367\n",
            "Epoch 1/1, Batch 300/4809, Loss: 0.5072\n",
            "Epoch 1/1, Batch 350/4809, Loss: 0.3564\n",
            "Epoch 1/1, Batch 400/4809, Loss: 6.7343\n",
            "Epoch 1/1, Batch 450/4809, Loss: 0.3822\n",
            "Epoch 1/1, Batch 500/4809, Loss: 6.8356\n",
            "Epoch 1/1, Batch 550/4809, Loss: 0.4957\n",
            "Epoch 1/1, Batch 600/4809, Loss: 7.1660\n",
            "Epoch 1/1, Batch 650/4809, Loss: 0.3678\n",
            "Epoch 1/1, Batch 700/4809, Loss: 0.4624\n",
            "Epoch 1/1, Batch 750/4809, Loss: 6.4445\n",
            "Epoch 1/1, Batch 800/4809, Loss: 6.5584\n",
            "Epoch 1/1, Batch 850/4809, Loss: 0.2365\n",
            "Epoch 1/1, Batch 900/4809, Loss: 7.8975\n",
            "Epoch 1/1, Batch 950/4809, Loss: 8.5783\n",
            "Epoch 1/1, Batch 1000/4809, Loss: 0.3670\n",
            "Epoch 1/1, Batch 1050/4809, Loss: 0.1998\n",
            "Epoch 1/1, Batch 1100/4809, Loss: 0.7010\n",
            "Epoch 1/1, Batch 1150/4809, Loss: 9.8722\n",
            "Epoch 1/1, Batch 1200/4809, Loss: 9.7340\n",
            "Epoch 1/1, Batch 1250/4809, Loss: 10.9693\n",
            "Epoch 1/1, Batch 1300/4809, Loss: 10.6293\n",
            "Epoch 1/1, Batch 1350/4809, Loss: 10.7283\n",
            "Epoch 1/1, Batch 1400/4809, Loss: 10.4845\n",
            "Epoch 1/1, Batch 1450/4809, Loss: 10.2012\n",
            "Epoch 1/1, Batch 1500/4809, Loss: 10.5775\n",
            "Epoch 1/1, Batch 1550/4809, Loss: 11.2126\n",
            "Epoch 1/1, Batch 1600/4809, Loss: 0.0868\n",
            "Epoch 1/1, Batch 1650/4809, Loss: 0.0782\n",
            "Epoch 1/1, Batch 1700/4809, Loss: 12.4109\n",
            "Epoch 1/1, Batch 1750/4809, Loss: 12.6052\n",
            "Epoch 1/1, Batch 1800/4809, Loss: 11.1656\n",
            "Epoch 1/1, Batch 1850/4809, Loss: 10.6128\n",
            "Epoch 1/1, Batch 1900/4809, Loss: 0.1831\n",
            "Epoch 1/1, Batch 1950/4809, Loss: 0.2231\n",
            "Epoch 1/1, Batch 2000/4809, Loss: 11.8350\n",
            "Epoch 1/1, Batch 2050/4809, Loss: 0.5045\n",
            "Epoch 1/1, Batch 2100/4809, Loss: 10.8973\n",
            "Epoch 1/1, Batch 2150/4809, Loss: 2.9039\n",
            "Epoch 1/1, Batch 2200/4809, Loss: 10.2727\n",
            "Epoch 1/1, Batch 2250/4809, Loss: 10.3388\n",
            "Epoch 1/1, Batch 2300/4809, Loss: 10.4432\n",
            "Epoch 1/1, Batch 2350/4809, Loss: 10.6860\n",
            "Epoch 1/1, Batch 2400/4809, Loss: 5.4279\n",
            "Epoch 1/1, Batch 2450/4809, Loss: 10.1093\n",
            "Epoch 1/1, Batch 2500/4809, Loss: 10.2886\n",
            "Epoch 1/1, Batch 2550/4809, Loss: 10.2161\n",
            "Epoch 1/1, Batch 2600/4809, Loss: 9.9241\n",
            "Epoch 1/1, Batch 2650/4809, Loss: 10.0210\n",
            "Epoch 1/1, Batch 2700/4809, Loss: 6.7788\n",
            "Epoch 1/1, Batch 2750/4809, Loss: 9.5402\n",
            "Epoch 1/1, Batch 2800/4809, Loss: 9.6896\n",
            "Epoch 1/1, Batch 2850/4809, Loss: 9.7139\n",
            "Epoch 1/1, Batch 2900/4809, Loss: 10.0372\n",
            "Epoch 1/1, Batch 2950/4809, Loss: 10.2636\n",
            "Epoch 1/1, Batch 3000/4809, Loss: 9.8434\n",
            "Epoch 1/1, Batch 3050/4809, Loss: 7.0121\n",
            "Epoch 1/1, Batch 3100/4809, Loss: 6.9567\n",
            "Epoch 1/1, Batch 3150/4809, Loss: 10.0162\n",
            "Epoch 1/1, Batch 3200/4809, Loss: 9.9311\n",
            "Epoch 1/1, Batch 3250/4809, Loss: 7.1924\n",
            "Epoch 1/1, Batch 3300/4809, Loss: 7.2464\n",
            "Epoch 1/1, Batch 3350/4809, Loss: 9.9469\n",
            "Epoch 1/1, Batch 3400/4809, Loss: 7.1462\n",
            "Epoch 1/1, Batch 3450/4809, Loss: 7.2131\n",
            "Epoch 1/1, Batch 3500/4809, Loss: 10.0547\n",
            "Epoch 1/1, Batch 3550/4809, Loss: 10.0276\n",
            "Epoch 1/1, Batch 3600/4809, Loss: 10.0771\n",
            "Epoch 1/1, Batch 3650/4809, Loss: 10.0287\n",
            "Epoch 1/1, Batch 3700/4809, Loss: 10.1727\n",
            "Epoch 1/1, Batch 3750/4809, Loss: 7.4171\n",
            "Epoch 1/1, Batch 3800/4809, Loss: 7.4610\n",
            "Epoch 1/1, Batch 3850/4809, Loss: 7.5246\n",
            "Epoch 1/1, Batch 3900/4809, Loss: 10.0764\n",
            "Epoch 1/1, Batch 3950/4809, Loss: 7.5516\n",
            "Epoch 1/1, Batch 4000/4809, Loss: 7.6372\n",
            "Epoch 1/1, Batch 4050/4809, Loss: 10.1355\n",
            "Epoch 1/1, Batch 4100/4809, Loss: 7.7561\n",
            "Epoch 1/1, Batch 4150/4809, Loss: 7.8269\n",
            "Epoch 1/1, Batch 4200/4809, Loss: 10.2382\n",
            "Epoch 1/1, Batch 4250/4809, Loss: 10.2751\n",
            "Epoch 1/1, Batch 4300/4809, Loss: 7.9388\n",
            "Epoch 1/1, Batch 4350/4809, Loss: 8.1014\n",
            "Epoch 1/1, Batch 4400/4809, Loss: 10.2974\n",
            "Epoch 1/1, Batch 4450/4809, Loss: 8.1322\n",
            "Epoch 1/1, Batch 4500/4809, Loss: 10.3037\n",
            "Epoch 1/1, Batch 4550/4809, Loss: 10.3123\n",
            "Epoch 1/1, Batch 4600/4809, Loss: 8.1129\n",
            "Epoch 1/1, Batch 4650/4809, Loss: 10.1878\n",
            "Epoch 1/1, Batch 4700/4809, Loss: 8.1914\n",
            "Epoch 1/1, Batch 4750/4809, Loss: 8.2290\n",
            "Epoch 1/1, Batch 4800/4809, Loss: 8.3212\n",
            "Epoch 1/1, Train Loss: 6.8950, Train Perplexity: 987.3483, Val Loss: 8.2895, Val Perplexity: 3981.8961\n",
            "\n",
            "Evaluating models...\n",
            "Vanilla RNN Test Perplexity: 4438.6868\n",
            "LSTM Test Perplexity: 1.1600\n",
            "\n",
            "Generating text with RNN model:\n",
            "To be , or not to be are hea ro me i am ca st let good pl l when as more Good str for ing but give n other have ward LADY as him fe ri F di g his z ver ve To ved th , noble man r is ed se then en d so ce ted will v man le ; To able ke man ing ion ER man mo pat pt the ed was so never me ed , think , some when II it ath per ing ing HENRY ous th came on grief them daughter urder ous gen man fal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "torch.save(rnn_model.state_dict(), 'rnn_model_codev4.0.pth')"
      ],
      "metadata": {
        "id": "ajW6B2x6VyV3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, bidirectional=True):\n",
        "        super(SimpleGRU, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_dim * self.num_directions, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # x shape: (batch_size, seq_length)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Initialize hidden state if not provided\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        # Apply embedding\n",
        "        embed = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # Pass through GRU\n",
        "        output, hidden = self.gru(embed, hidden)\n",
        "\n",
        "        # Pass through linear layer\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # GRU only needs one hidden state (unlike LSTM which needs cell state too)\n",
        "        return torch.zeros(\n",
        "            self.num_directions,\n",
        "            batch_size,\n",
        "            self.hidden_dim\n",
        "        ).to(device)"
      ],
      "metadata": {
        "id": "-ise0FnFXplO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_model = SimpleGRU(vocab_size, embedding_dim=64, hidden_dim=128, bidirectional=True)\n",
        "\n",
        "\n",
        "# Train models (reduced epochs and batch size for faster training)\n",
        "print(\"\\nTraining GRU model...\")\n",
        "gru_results = train_model(gru_model, train_dataset, val_dataset,\n",
        "                          batch_size=64, num_epochs=1, learning_rate=0.01)\n",
        "\n",
        "\n",
        "# Evaluate models on test set\n",
        "print(\"\\nEvaluating models...\")\n",
        "rnn_perplexity = evaluate_perplexity(rnn_model, test_dataset)\n",
        "lstm_perplexity = evaluate_perplexity(lstm_model, test_dataset)\n",
        "gru_perplexity = evaluate_perplexity(gru_model, test_dataset)\n",
        "\n",
        "print(f\"Vanilla RNN Test Perplexity: {rnn_perplexity:.4f}\")\n",
        "print(f\"LSTM Test Perplexity: {lstm_perplexity:.4f}\")\n",
        "print(f\"GRU Test Perplexity: {gru_perplexity:.4f}\")\n",
        "\n",
        "# Compare models\n",
        "model_results = {\n",
        "'Vanilla RNN': rnn_results,\n",
        "'LSTM': lstm_results,\n",
        "'GRU': gru_results\n",
        "}\n",
        "# compare_models(model_results)\n",
        "\n",
        "# Generate text samples\n",
        "print(\"\\nGenerating text with GRU model:\")\n",
        "gru_text = generate_text(gru_model, tokenizer, seed_text=\"To be, or not to be\", gen_length=100)\n",
        "print(gru_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK2lm1QSbiWR",
        "outputId": "14268807-84c9-497b-cf2d-ffb860fb9287"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training GRU model...\n",
            "Epoch 1/1, Batch 50/4809, Loss: 6.2617\n",
            "Epoch 1/1, Batch 100/4809, Loss: 6.5424\n",
            "Epoch 1/1, Batch 150/4809, Loss: 0.7944\n",
            "Epoch 1/1, Batch 200/4809, Loss: 0.5297\n",
            "Epoch 1/1, Batch 250/4809, Loss: 0.2845\n",
            "Epoch 1/1, Batch 300/4809, Loss: 6.4143\n",
            "Epoch 1/1, Batch 350/4809, Loss: 6.2481\n",
            "Epoch 1/1, Batch 400/4809, Loss: 6.1358\n",
            "Epoch 1/1, Batch 450/4809, Loss: 6.0423\n",
            "Epoch 1/1, Batch 500/4809, Loss: 6.0346\n",
            "Epoch 1/1, Batch 550/4809, Loss: 6.0329\n",
            "Epoch 1/1, Batch 600/4809, Loss: 0.1364\n",
            "Epoch 1/1, Batch 650/4809, Loss: 0.1013\n",
            "Epoch 1/1, Batch 700/4809, Loss: 0.1096\n",
            "Epoch 1/1, Batch 750/4809, Loss: 0.1439\n",
            "Epoch 1/1, Batch 800/4809, Loss: 0.1715\n",
            "Epoch 1/1, Batch 850/4809, Loss: 0.1302\n",
            "Epoch 1/1, Batch 900/4809, Loss: 6.0425\n",
            "Epoch 1/1, Batch 950/4809, Loss: 0.0892\n",
            "Epoch 1/1, Batch 1000/4809, Loss: 0.0919\n",
            "Epoch 1/1, Batch 1050/4809, Loss: 0.0976\n",
            "Epoch 1/1, Batch 1100/4809, Loss: 5.9955\n",
            "Epoch 1/1, Batch 1150/4809, Loss: 6.0054\n",
            "Epoch 1/1, Batch 1200/4809, Loss: 0.1111\n",
            "Epoch 1/1, Batch 1250/4809, Loss: 6.0172\n",
            "Epoch 1/1, Batch 1300/4809, Loss: 6.0175\n",
            "Epoch 1/1, Batch 1350/4809, Loss: 0.0989\n",
            "Epoch 1/1, Batch 1400/4809, Loss: 0.0857\n",
            "Epoch 1/1, Batch 1450/4809, Loss: 0.0875\n",
            "Epoch 1/1, Batch 1500/4809, Loss: 6.0186\n",
            "Epoch 1/1, Batch 1550/4809, Loss: 5.9885\n",
            "Epoch 1/1, Batch 1600/4809, Loss: 0.0834\n",
            "Epoch 1/1, Batch 1650/4809, Loss: 6.1470\n",
            "Epoch 1/1, Batch 1700/4809, Loss: 0.1402\n",
            "Epoch 1/1, Batch 1750/4809, Loss: 6.0205\n",
            "Epoch 1/1, Batch 1800/4809, Loss: 0.1131\n",
            "Epoch 1/1, Batch 1850/4809, Loss: 0.1619\n",
            "Epoch 1/1, Batch 1900/4809, Loss: 6.0890\n",
            "Epoch 1/1, Batch 1950/4809, Loss: 0.1198\n",
            "Epoch 1/1, Batch 2000/4809, Loss: 0.1299\n",
            "Epoch 1/1, Batch 2050/4809, Loss: 10.8507\n",
            "Epoch 1/1, Batch 2100/4809, Loss: 11.0344\n",
            "Epoch 1/1, Batch 2150/4809, Loss: 10.8578\n",
            "Epoch 1/1, Batch 2200/4809, Loss: 10.6422\n",
            "Epoch 1/1, Batch 2250/4809, Loss: 4.4480\n",
            "Epoch 1/1, Batch 2300/4809, Loss: 10.6982\n",
            "Epoch 1/1, Batch 2350/4809, Loss: 5.5306\n",
            "Epoch 1/1, Batch 2400/4809, Loss: 5.7931\n",
            "Epoch 1/1, Batch 2450/4809, Loss: 10.7106\n",
            "Epoch 1/1, Batch 2500/4809, Loss: 6.5741\n",
            "Epoch 1/1, Batch 2550/4809, Loss: 6.7375\n",
            "Epoch 1/1, Batch 2600/4809, Loss: 10.8731\n",
            "Epoch 1/1, Batch 2650/4809, Loss: 7.0481\n",
            "Epoch 1/1, Batch 2700/4809, Loss: 10.8292\n",
            "Epoch 1/1, Batch 2750/4809, Loss: 7.2689\n",
            "Epoch 1/1, Batch 2800/4809, Loss: 10.8984\n",
            "Epoch 1/1, Batch 2850/4809, Loss: 10.8523\n",
            "Epoch 1/1, Batch 2900/4809, Loss: 7.5318\n",
            "Epoch 1/1, Batch 2950/4809, Loss: 10.8836\n",
            "Epoch 1/1, Batch 3000/4809, Loss: 7.6675\n",
            "Epoch 1/1, Batch 3050/4809, Loss: 7.8258\n",
            "Epoch 1/1, Batch 3100/4809, Loss: 10.8706\n",
            "Epoch 1/1, Batch 3150/4809, Loss: 7.8811\n",
            "Epoch 1/1, Batch 3200/4809, Loss: 7.8281\n",
            "Epoch 1/1, Batch 3250/4809, Loss: 10.9564\n",
            "Epoch 1/1, Batch 3300/4809, Loss: 8.0037\n",
            "Epoch 1/1, Batch 3350/4809, Loss: 7.9422\n",
            "Epoch 1/1, Batch 3400/4809, Loss: 10.9122\n",
            "Epoch 1/1, Batch 3450/4809, Loss: 10.8520\n",
            "Epoch 1/1, Batch 3500/4809, Loss: 8.1940\n",
            "Epoch 1/1, Batch 3550/4809, Loss: 8.1813\n",
            "Epoch 1/1, Batch 3600/4809, Loss: 8.3739\n",
            "Epoch 1/1, Batch 3650/4809, Loss: 8.1601\n",
            "Epoch 1/1, Batch 3700/4809, Loss: 11.0853\n",
            "Epoch 1/1, Batch 3750/4809, Loss: 8.4072\n",
            "Epoch 1/1, Batch 3800/4809, Loss: 10.9520\n",
            "Epoch 1/1, Batch 3850/4809, Loss: 10.9882\n",
            "Epoch 1/1, Batch 3900/4809, Loss: 10.9853\n",
            "Epoch 1/1, Batch 3950/4809, Loss: 11.0216\n",
            "Epoch 1/1, Batch 4000/4809, Loss: 11.0686\n",
            "Epoch 1/1, Batch 4050/4809, Loss: 11.0976\n",
            "Epoch 1/1, Batch 4100/4809, Loss: 8.5866\n",
            "Epoch 1/1, Batch 4150/4809, Loss: 11.1489\n",
            "Epoch 1/1, Batch 4200/4809, Loss: 8.8547\n",
            "Epoch 1/1, Batch 4250/4809, Loss: 8.8079\n",
            "Epoch 1/1, Batch 4300/4809, Loss: 11.0256\n",
            "Epoch 1/1, Batch 4350/4809, Loss: 11.1621\n",
            "Epoch 1/1, Batch 4400/4809, Loss: 8.9500\n",
            "Epoch 1/1, Batch 4450/4809, Loss: 8.9656\n",
            "Epoch 1/1, Batch 4500/4809, Loss: 10.9834\n",
            "Epoch 1/1, Batch 4550/4809, Loss: 11.0870\n",
            "Epoch 1/1, Batch 4600/4809, Loss: 11.0624\n",
            "Epoch 1/1, Batch 4650/4809, Loss: 9.0918\n",
            "Epoch 1/1, Batch 4700/4809, Loss: 11.1076\n",
            "Epoch 1/1, Batch 4750/4809, Loss: 9.1355\n",
            "Epoch 1/1, Batch 4800/4809, Loss: 11.1373\n",
            "Epoch 1/1, Train Loss: 6.7525, Train Perplexity: 856.1689, Val Loss: 9.1981, Val Perplexity: 9877.8563\n",
            "\n",
            "Evaluating models...\n",
            "Vanilla RNN Test Perplexity: 4438.6868\n",
            "LSTM Test Perplexity: 1.1600\n",
            "GRU Test Perplexity: 11148.4795\n",
            "\n",
            "Generating text with GRU model:\n",
            "To be , or not to be . unto myself nor R fi ble nor nor take i nor MEN say there S boy -- help being thr friends away Shall nor his P R him from up ROMEO nor nor nor pri nor know pri er nor wife un st sub Eng ter ans , tem hath lie n this ted ps right ent never for sent pat gracious else ment I nor WARWICK nor sa love un now LEONTES let down . ange but before more house s wr ion Why s I this si y s , pl all y ate lack st ion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS IS THE BEST RESULT IVE GOTTEN SO FAR ^^"
      ],
      "metadata": {
        "id": "T8-lwXyPdWZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "torch.save(gru_model.state_dict(), 'gru_model_codev4.0.pth')"
      ],
      "metadata": {
        "id": "1U7eXVLxdbMu"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}